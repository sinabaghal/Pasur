{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "def find_subsets_with_sum(values, r, target_sum):\n",
    "    subsets = []\n",
    "    \n",
    "    for subset in combinations(values, r):\n",
    "        if sum(subset) == target_sum:\n",
    "            subsets.append(subset)\n",
    "    return subsets\n",
    "\n",
    "def look_up_func(hand,ranks):\n",
    "\n",
    "    values = [i+1 for i, count in enumerate(ranks) for _ in range(count)]\n",
    "    return [find_subsets_with_sum(values,r,11-int(hand)) for r in range(1, 8)]\n",
    "    \n",
    "\n",
    "\n",
    "hand = '3'\n",
    "ranks = '4002000100'\n",
    "\n",
    "# HANDS = [f'{i}' for i in range(1,11)]\n",
    "# RANKS = list(itertools.product(range(5), repeat=10))\n",
    "\n",
    "def look_up_t(hand, ranks):\n",
    "\n",
    "    ranks_digit = [int(ranks[i]) for i in range(10)]\n",
    "    look_up_t = [find_subsets_with_sum(ranks_digit,r,11-int(hand)) for r in range(1, 8)]\n",
    "    pools = []\n",
    "    hist = []\n",
    "    for item in look_up_t:\n",
    "\n",
    "        if len(item)>0:\n",
    "            for sub_item in item:\n",
    "                print(sub_item)\n",
    "                sub_item = sorted([f'{i}' for i in sub_item])\n",
    "                if sub_item not in hist:\n",
    "                    hist.append(sub_item)\n",
    "                    print(sub_item)\n",
    "                    pools.append(dict(Counter(sub_item)))\n",
    "    \n",
    "\n",
    "    return pools \n",
    "\n",
    "look_up_t(hand, ranks)\n",
    "# res = [\n",
    "#     {str(key): value for key, value in Counter(subset).items()}\n",
    "#     for subsets in look_up_t\n",
    "#     for subset in subsets\n",
    "# ]\n",
    "    \n",
    "    \n",
    "# res = {\n",
    "#     hand: {\n",
    "#         ranks: look_up_func(hand, ranks)\n",
    "#         for ranks in RANKS if ranks[hand] < 4\n",
    "#     }\n",
    "#     for hand in HANDS\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch \n",
    "c_edg = torch.tensor([2,2,2,3,3])\n",
    "t_edg = torch.tensor([0,1,2,0,1,2,3,4,3,4,3,4])\n",
    "t_sgm = torch.tensor([1,1,1,1,1,1,1,1,1,1,1,1], dtype=torch.float32)\n",
    "t_inv = torch.argsort(t_edg,stable=True)\n",
    "t_idx = torch.empty_like(t_inv)\n",
    "t_idx[t_inv]  = torch.arange(len(t_inv))\n",
    "i_max         = c_edg.max()\n",
    "t_msk         = torch.arange(i_max).unsqueeze(0) < c_edg.unsqueeze(1)\n",
    "t_mtx         = torch.zeros_like(t_msk, dtype=t_sgm.dtype)\n",
    "t_mtx[t_msk]  = t_sgm[t_inv]\n",
    "t_smp         = torch.multinomial(t_mtx, num_samples=1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 981165 bytes (0.94 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = '../STRG/nns/CFR0_TRV0_EXT0.pt.zst'\n",
    "size_bytes = os.path.getsize(file_path)\n",
    "size_mb = size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"File size: {size_bytes} bytes ({size_mb:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1: 1 files, 42.78 MB\n",
      "Group 2: 3 files, 38.58 MB\n",
      "Group 3: 4 files, 34.80 MB\n",
      "Group 4: 9 files, 38.41 MB\n",
      "Group 5: 14 files, 38.51 MB\n",
      "Group 6: 18 files, 38.26 MB\n",
      "Group 7: 24 files, 39.81 MB\n",
      "Group 8: 32 files, 39.98 MB\n",
      "Group 9: 51 files, 39.65 MB\n",
      "Group 10: 44 files, 16.49 MB\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from Utils import partition_folder\n",
    "path = '../ADVT/nns/'\n",
    "gp = partition_folder(path,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['../ADVT/nns/CFR5_TRV1_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR0_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV3_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR3_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV5_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR0_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV1_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR4_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV5_EXT0.pt.zst'],\n",
       " ['../ADVT/nns/CFR5_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV1_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR1_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV4_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR4_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV4_EXT1.pt.zst'],\n",
       " ['../ADVT/nns/CFR2_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV3_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV8_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV3_EXT0.pt.zst'],\n",
       " ['../ADVT/nns/CFR8_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV3_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV4_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV9_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV7_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR4_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR7_TRV1_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV5_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR3_TRV4_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV6_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV1_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV5_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV8_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR0_TRV7_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV0_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV6_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR2_TRV2_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR1_TRV9_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR8_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR9_TRV0_EXT1.pt.zst',\n",
       "  '../ADVT/nns/CFR5_TRV2_EXT0.pt.zst',\n",
       "  '../ADVT/nns/CFR6_TRV3_EXT0.pt.zst']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(2)     # t1 = tensor([1., 1.])\n",
    "t2 = t1                # t2 now points to the same tensor as t1\n",
    "t1 = torch.zeros(2)    # t1 is now reassigned to a new tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "N = 3  # Example size\n",
    "tensor = torch.randint(low=0, high=2, size=(N, 4, 5)) # Replace with your tensor\n",
    "\n",
    "# Reshape to (N * 52, 4) to get all (4,) tensors\n",
    "# Each tensor along dim 1 for each N and 52 is now a row\n",
    "reshaped = tensor.permute(0, 2, 1).reshape(-1, 4)  # Shape: (N * 52, 4)\n",
    "\n",
    "# .permute(0, 2, 1).reshape(-1, 4).unique(reshaped, dim=0)\n",
    "# Find unique tensors\n",
    "unique_tensors = torch.unique(reshaped, dim=0)  # Shape: (M, 4), where M is the number of unique tensors\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of unique tensors: {unique_tensors.shape[0]}\")\n",
    "print(\"Unique tensors:\\n\", unique_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    for j in range(5):\n",
    "\n",
    "        print(tensor[i,:,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crd_cntr = {'4': {'4♠', '4♣'}, '5': {'5♣'}} \n",
    "pool = {'5': 1, '4': 1}\n",
    "# *[list(itertools.combinations(crd_cntr[key], pool[key]))\n",
    "#                              for key in crd_cntr.keys()]\n",
    "\n",
    "[set(itertools.chain.from_iterable(nested_tuple)) \n",
    "        for nested_tuple in \n",
    "        itertools.product(*[itertools.combinations(crd_cntr[key], pool[key]) \n",
    "                             for key in crd_cntr.keys()])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "indices = torch.tensor([0, 1], device='cuda:0')\n",
    "for start, end in zip(indices[:-1], indices[1:]): \n",
    "    print(start)\n",
    "    print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hand = '3'\n",
    "ranks = '4002000100'\n",
    "\n",
    "values = [i+1 for i, count in enumerate(ranks_digit) for _ in range(count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "TYPE = torch.uint8\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "score = torch.zeros((1,8),dtype=TYPE, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [[0, 1], [0, 1, 2]]  # Example input\n",
    "\n",
    "# Step 1: Convert list to dictionaries\n",
    "dict_list = [{val: idx for idx, val in enumerate(sublist)} for sublist in lst]\n",
    "\n",
    "# Step 2: Compute the union of all sets of dictionary keys\n",
    "union_set = set().union(*[d.keys() for d in dict_list])\n",
    "\n",
    "print(\"List of Dictionaries:\", dict_list)\n",
    "print(\"Union of All Sets:\", union_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dictionary\n",
    "crd_cntr = {'1': [1, 2, 3, 4], '4': [1, 2], '8': [1]}\n",
    "\n",
    "# Generate the ranks list with list comprehension\n",
    "ranks = ''.join(map(str, [len(crd_cntr.get(str(i), [])) for i in range(1,11)]))\n",
    "\n",
    "print(ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Initialization\n",
    "HANDS = [f'{i}' for i in range(1, 11)]\n",
    "RANKS = [dict(zip(HANDS, x)) for x in itertools.product(range(5), repeat=10)]\n",
    "res = {hand: {} for hand in HANDS}\n",
    "\n",
    "# Vectorized dictionary comprehension\n",
    "res = {\n",
    "    hand: {\n",
    "        ranks: look_up_func(hand, ranks)\n",
    "        for ranks in RANKS if ranks[hand] < 4\n",
    "    }\n",
    "    for hand in HANDS\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor (4, 3, 2)\n",
    "Z = torch.tensor([\n",
    "    [[1, 2], [3, 4], [5, 6]],\n",
    "    [[1, 2], [7, 8], [5, 6]],\n",
    "    [[9, 10], [3, 4], [11, 12]],\n",
    "    [[9, 10], [7, 8], [13, 14]]\n",
    "])\n",
    "\n",
    "# Step 1: Reshape to (12, 2) for unique computation\n",
    "flattened_Z = Z.reshape(-1, Z.size(-1))  # (12, 2)\n",
    "\n",
    "# Step 2: Find unique rows (dim=0) and inverse indices\n",
    "unique_rows, inverse_indices = torch.unique(\n",
    "    flattened_Z,\n",
    "    dim=0,\n",
    "    return_inverse=True\n",
    ")\n",
    "\n",
    "# Step 3: Reshape inverse_indices back to (4, 3)\n",
    "unique_indices_original_shape = inverse_indices.reshape(Z.shape[0], Z.shape[1])\n",
    "\n",
    "print(\"Unique rows (no duplicates):\\n\", unique_rows)\n",
    "print(\"Original tensor's row-to-unique mapping:\\n\", unique_indices_original_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_indices_original_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_numeric_cards = ['2a','2b']\n",
    "from collections import Counter\n",
    "pool_numeric_cards_ranks = Counter(card[0] for card in pool_numeric_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "king_cards = ['K']\n",
    "pool_king_cards = ['K0','K1']\n",
    "\n",
    "p = 0\n",
    "[{f'p{p}_side':hand, f'pool_side':set(pool)} for hand,pool in itertools.product(king_cards,pool_king_cards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "pool_num = ['2a','2b','3a']\n",
    "ranks = {card[0]:card[1] for card in pool_num}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "ranks = defaultdict(set)  # Each key maps to a set\n",
    "ranks['2'].add('a')       # This works because ranks['2'] is a set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crd_cntr = {'1':{'1a','1b'},'2':{'2a'},'3':{'3a','3b'}}\n",
    "ranks = {'1':1,'2':1,'3':2}\n",
    "xx = [tuple(itertools.chain.from_iterable(nested_tuple)) \n",
    "      for nested_tuple in itertools.product(*[itertools.combinations(crd_cntr[f'{key}'], pool[f'{key}']) for key in range(1,4)])]\n",
    "\n",
    "# [list(itertools.combinations(crd_cntr[key], value)) for key,value in pool.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Input dictionary\n",
    "# x = {'0': 3, '1': 2, '2': 1, '3': 2, '4': 1, '5': 1, '6': 1, '7': 1, '8': 1, '9': 1, '10': 1}\n",
    "x = {'3': 2, '4': 1, '5': 1, '6': 1, '7': 1}\n",
    "hand = 11\n",
    "\n",
    "# Create a list of possible values for each index (0 to 10) based on x[i]\n",
    "ranges = [range(x[key] + 1) for key in x.keys()]\n",
    "\n",
    "# Generate all possible combinations of values\n",
    "valid_combinations = []\n",
    "for combination in product(*ranges):\n",
    "    if sum(idx * count for idx, count in enumerate(combination)) == hand:\n",
    "        valid_combinations.append(combination)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Number of valid combinations: {len(valid_combinations)}\")\n",
    "for comb in valid_combinations:\n",
    "    print(comb)\n",
    "\n",
    "\n",
    "aa.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "K = 10 \n",
    "tt = torch.zeros(K, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "sum([np.log(i) for i in range(1,53)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensors (shape [2, 4])\n",
    "input_tensor = torch.tensor([[0, 1, 1, 0],\n",
    "                             [1, 0, 0, 1]], device='cuda')  # Move to GPU\n",
    "\n",
    "# Identify indices where elements are 1\n",
    "indices1 = (input_tensor[0] == 1).nonzero(as_tuple=False).squeeze(-1)\n",
    "indices2 = (input_tensor[1] == 1).nonzero(as_tuple=False).squeeze(-1)\n",
    "\n",
    "grid1, grid2 = torch.meshgrid(indices1, indices2, indexing='ij')\n",
    "combinations = torch.stack([grid1.flatten(), grid2.flatten()], dim=-1)\n",
    "num_combinations = combinations.size(0)\n",
    "output_tensor = torch.zeros((num_combinations, *input_tensor.shape), device='cuda')\n",
    "\n",
    "rows = torch.arange(num_combinations, device='cuda')\n",
    "output_tensor[rows, 0, combinations[:, 0]] = 1\n",
    "output_tensor[rows, 1, combinations[:, 1]] = 1\n",
    "\n",
    "# Print results\n",
    "output_tensor[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor (shape [2, 8])\n",
    "input_tensor = torch.tensor([[0, 1, 1, 0, 0, 1, 1, 0],\n",
    "                             [1, 0, 0, 1, 1, 0, 0, 1]], device='cuda')  # On GPU\n",
    "\n",
    "# Define the groups\n",
    "groups = [slice(0, 4), slice(4, 8)]  # Define the column slices for groups\n",
    "\n",
    "# Process both groups simultaneously\n",
    "indices = [(input_tensor[:, group] == 1).nonzero(as_tuple=False) for group in groups]\n",
    "\n",
    "# Separate indices for rows and columns for each group\n",
    "indices1_row, indices1_col = indices[0][:, 0], indices[0][:, 1]\n",
    "indices2_row, indices2_col = indices[1][:, 0], indices[1][:, 1]\n",
    "\n",
    "# Generate all combinations of columns within each group\n",
    "grid1_col, grid2_col = torch.meshgrid(indices1_col, indices2_col, indexing='ij')\n",
    "grid1_row, grid2_row = torch.meshgrid(indices1_row, indices2_row, indexing='ij')\n",
    "\n",
    "# Flatten the grids to get all combinations\n",
    "grid1_flat = torch.stack([grid1_row.flatten(), grid1_col.flatten()], dim=1)\n",
    "grid2_flat = torch.stack([grid2_row.flatten(), grid2_col.flatten()], dim=1)\n",
    "\n",
    "# Prepare output tensors\n",
    "num_combinations = grid1_flat.size(0)\n",
    "output = torch.zeros((num_combinations, 2, 8), device='cuda')\n",
    "\n",
    "# Scatter values for both groups\n",
    "output[torch.arange(num_combinations), grid1_flat[:, 0], grid1_flat[:, 1]] = 1\n",
    "output[torch.arange(num_combinations), grid2_flat[:, 0], grid2_flat[:, 1] + 4] = 1\n",
    "\n",
    "# Print results\n",
    "output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor (shape [2, 8])\n",
    "input_tensor = torch.tensor([[0, 1, 1, 0, 0, 1, 1, 0],\n",
    "                             [1, 0, 0, 1, 1, 0, 0, 1]], device='cuda')  # On GPU\n",
    "\n",
    "# Define the groups\n",
    "groups = [slice(0, 4), slice(4, 8)]  # Define the column slices for groups\n",
    "\n",
    "# Process both groups to find the indices where elements are 1\n",
    "indices = [(input_tensor[:, group] == 1).nonzero(as_tuple=False) for group in groups]\n",
    "\n",
    "# Stack all indices together for efficient computation\n",
    "group1_indices = indices[0]\n",
    "group2_indices = indices[1]\n",
    "\n",
    "# Generate all combinations of indices using broadcasting\n",
    "group1_indices = group1_indices[:, None, :]  # Add a new dimension for broadcasting\n",
    "group2_indices = group2_indices[None, :, :]  # Add a new dimension for broadcasting\n",
    "\n",
    "all_combinations = torch.cat([\n",
    "    group1_indices.expand(-1, group2_indices.size(1), -1),\n",
    "    group2_indices.expand(group1_indices.size(0), -1, -1)\n",
    "], dim=-1)  # Shape: [num_combinations_group1, num_combinations_group2, 4]\n",
    "\n",
    "# Reshape combinations to flatten\n",
    "all_combinations = all_combinations.view(-1, 4)\n",
    "\n",
    "# Prepare output tensor\n",
    "num_combinations = all_combinations.size(0)\n",
    "output = torch.zeros((num_combinations, 2, 8), device='cuda')\n",
    "\n",
    "# Scatter values for both groups in the output tensor\n",
    "output[torch.arange(num_combinations), all_combinations[:, 0], all_combinations[:, 1]] = 1\n",
    "output[torch.arange(num_combinations), all_combinations[:, 2], all_combinations[:, 3] + 4] = 1\n",
    "\n",
    "# Print results\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor of shape [2, 8] on GPU\n",
    "input_tensor = torch.tensor([\n",
    "    [0, 1, 1, 0, 0, 1, 1, 0],\n",
    "    [1, 0, 0, 1, 1, 0, 0, 1]\n",
    "], device='cuda')\n",
    "\n",
    "# Define the column slices for the two groups\n",
    "groups = [slice(0, 4), slice(4, 8)]\n",
    "\n",
    "# For each group, find indices (row, col_in_group) where entries are 1\n",
    "indices = [\n",
    "    (input_tensor[:, grp] == 1).nonzero(as_tuple=False)\n",
    "    for grp in groups\n",
    "]\n",
    "# indices[0] -> shape [N1, 2], each row is [row, col_in_group_0..3]\n",
    "# indices[1] -> shape [N2, 2], each row is [row, col_in_group_0..3]\n",
    "\n",
    "group1_indices = indices[0]  # [N1, 2]\n",
    "group2_indices = indices[1]  # [N2, 2]\n",
    "\n",
    "# We will combine every row from group1_indices with every row from group2_indices\n",
    "# Broadcasting approach:\n",
    "#   group1_indices => shape [N1, 2] -> [N1, 1, 2]\n",
    "#   group2_indices => shape [N2, 2] -> [1, N2, 2]\n",
    "# Expanding and concatenating them gives all combinations [N1, N2, 4].\n",
    "g1 = group1_indices[:, None, :]  # [N1, 1, 2]\n",
    "g2 = group2_indices[None, :, :]  # [1, N2, 2]\n",
    "\n",
    "# Concatenate along the last dim -> [N1, N2, 4]\n",
    "# Each entry looks like [row1, col1, row2, col2]\n",
    "all_combinations = torch.cat([g1.expand(-1, g2.size(1), -1),\n",
    "                              g2.expand(g1.size(0), -1, -1)], dim=-1)\n",
    "\n",
    "# Reshape to [N1*N2, 4]\n",
    "all_combinations = all_combinations.reshape(-1, 4)\n",
    "num_combinations = all_combinations.size(0)\n",
    "\n",
    "# Prepare the output of shape [N1*N2, 2, 8]\n",
    "output = torch.zeros((num_combinations, 2, 8), device='cuda')\n",
    "\n",
    "# Unpack row/col info\n",
    "row1 = all_combinations[:, 0]  # row in [0..1]\n",
    "col1 = all_combinations[:, 1]  # col in [0..3] for group1\n",
    "row2 = all_combinations[:, 2]  # row in [0..1]\n",
    "col2 = all_combinations[:, 3]  # col in [0..3] for group2 (need +4 offset in output)\n",
    "\n",
    "# Use advanced indexing to set ones in the output\n",
    "idx = torch.arange(num_combinations, device='cuda')\n",
    "output[idx, row1, col1] = 1\n",
    "output[idx, row2, col2 + 4] = 1\n",
    "\n",
    "# Print or inspect the final result\n",
    "print(\"Shape of output:\", output.shape)  # Should be [N1*N2, 2, 8]\n",
    "output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor (shape [2, 8]) on GPU\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 1, 0, 0, 1, 1, 0],\n",
    "        [1, 0, 0, 1, 1, 0, 0, 1]\n",
    "    ],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# 1) Identify column indices where elements are 1 for each row\n",
    "indices_row0 = (input_tensor[0] == 1).nonzero(as_tuple=False).flatten()\n",
    "indices_row1 = (input_tensor[1] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "# 2) Generate all possible combinations of those column indices\n",
    "grid0, grid1 = torch.meshgrid(indices_row0, indices_row1, indexing='ij')\n",
    "combinations = torch.stack([grid0.flatten(), grid1.flatten()], dim=-1)\n",
    "num_combinations = combinations.size(0)\n",
    "\n",
    "# 3) Prepare the output tensor of shape [num_combinations, 2, 8]\n",
    "output_tensor = torch.zeros((num_combinations, 2, 8), device='cuda')\n",
    "\n",
    "# 4) Scatter values into the output tensor\n",
    "batch_indices = torch.arange(num_combinations, device='cuda')\n",
    "output_tensor[batch_indices, 0, combinations[:, 0]] = 1\n",
    "output_tensor[batch_indices, 1, combinations[:, 1]] = 1\n",
    "\n",
    "# Example: print the first combination\n",
    "print(\"Output shape:\", output_tensor.shape)    # e.g. [num_pairs, 2, 8]\n",
    "print(\"First combination:\\n\", output_tensor[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor on GPU (shape [2, 8])\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 1, 0, 0, 1, 1, 0],\n",
    "        [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "    ],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# Split columns into two groups:\n",
    "# Group A -> columns [0..3], Group B -> columns [4..7]\n",
    "# We'll gather all valid combinations for each group and then combine them.\n",
    "\n",
    "def get_group_combinations(tensor_2xM, col_start, col_end):\n",
    "    \"\"\"\n",
    "    Given a 2xM tensor, extract the columns [col_start:col_end],\n",
    "    find all combinations of 1's for row0 and row1 within that group,\n",
    "    and return a result of shape [num_combinations, 2, original_num_cols].\n",
    "\n",
    "    The returned tensor has zeros everywhere except in the group slice,\n",
    "    where each row has exactly one '1' at one of the valid column indices.\n",
    "    \"\"\"\n",
    "    # 1) Identify column indices (within group) where each row is 1\n",
    "    sub_tensor = tensor_2xM[:, col_start:col_end]  # shape [2, group_width]\n",
    "    indices_row0 = (sub_tensor[0] == 1).nonzero(as_tuple=False).flatten()\n",
    "    indices_row1 = (sub_tensor[1] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # 2) Generate all combinations of these column indices (no loops)\n",
    "    grid0, grid1 = torch.meshgrid(indices_row0, indices_row1, indexing='ij')\n",
    "    combos = torch.stack([grid0.flatten(), grid1.flatten()], dim=-1)\n",
    "    num_combos = combos.size(0)\n",
    "\n",
    "    # 3) Prepare an output of shape [num_combos, 2, original_num_cols], all zeros\n",
    "    original_num_cols = tensor_2xM.size(1)\n",
    "    output = torch.zeros((num_combos, 2, original_num_cols), device=tensor_2xM.device)\n",
    "\n",
    "    # 4) Scatter '1's into the correct columns, offset by col_start\n",
    "    # combos[:, 0] are valid columns for row 0 in the group slice\n",
    "    # combos[:, 1] are valid columns for row 1 in the group slice\n",
    "    batch_indices = torch.arange(num_combos, device=tensor_2xM.device)\n",
    "    output[batch_indices, 0, combos[:, 0] + col_start] = 1\n",
    "    output[batch_indices, 1, combos[:, 1] + col_start] = 1\n",
    "\n",
    "    return output\n",
    "\n",
    "# --- Compute Combinations for Each Group Separately ---\n",
    "\n",
    "# Group A: columns [0..3]\n",
    "groupA_output = get_group_combinations(input_tensor, col_start=0, col_end=4)\n",
    "# Group B: columns [4..7]\n",
    "groupB_output = get_group_combinations(input_tensor, col_start=4, col_end=8)\n",
    "\n",
    "# --- Concatenate Results Along the Batch Dimension ---\n",
    "# We simply stack all the valid combos from Group A and Group B\n",
    "final_output = torch.cat([groupA_output, groupB_output], dim=0)\n",
    "\n",
    "print(\"Final output shape:\", final_output.shape)\n",
    "# E.g. [num_combos_A + num_combos_B, 2, 8]\n",
    "\n",
    "# Let's look at the first few combos for illustration\n",
    "for i in range(min(8, final_output.size(0))):\n",
    "    print(f\"Combination {i}:\\n\", final_output[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose input_tensor has shape [2, 8], on CUDA\n",
    "input_tensor = torch.tensor([\n",
    "    [0, 1, 1, 0, 0, 1, 1, 0],  # row 0\n",
    "    [1, 1, 0, 0, 0, 0, 0, 0]   # row 1 with no 1s (example)\n",
    "], device='cuda')\n",
    "\n",
    "# Identify column indices where each row has 1\n",
    "indices_row0 = (input_tensor[0] == 1).nonzero(as_tuple=False).flatten()\n",
    "indices_row1 = (input_tensor[1] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "# If row1 has no '1's, we produce a one-sided output\n",
    "if indices_row1.numel() == 0:\n",
    "    # We have len(indices_row0) valid positions in row0\n",
    "    # For each position, create a slice with row0=1 at that position, row1=all zeros\n",
    "    num_combinations = indices_row0.size(0)\n",
    "    \n",
    "    # Prepare the output: [num_combinations, 2, 8]\n",
    "    output_tensor = torch.zeros((num_combinations, 2, input_tensor.shape[1]), device='cuda')\n",
    "    \n",
    "    batch_idx = torch.arange(num_combinations, device='cuda')\n",
    "    output_tensor[batch_idx, 0, indices_row0] = 1\n",
    "    # Row 1 stays zero, so nothing else to do\n",
    "\n",
    "else:\n",
    "    # Standard pairwise logic via meshgrid\n",
    "    grid0, grid1 = torch.meshgrid(indices_row0, indices_row1, indexing='ij')\n",
    "    combos = torch.stack([grid0.flatten(), grid1.flatten()], dim=-1)\n",
    "    num_combinations = combos.size(0)\n",
    "    \n",
    "    output_tensor = torch.zeros((num_combinations, 2, input_tensor.shape[1]), device='cuda')\n",
    "    batch_idx = torch.arange(num_combinations, device='cuda')\n",
    "    output_tensor[batch_idx, 0, combos[:, 0]] = 1\n",
    "    output_tensor[batch_idx, 1, combos[:, 1]] = 1\n",
    "\n",
    "# Print the results\n",
    "print(\"Output shape:\", output_tensor.shape)\n",
    "print(\"Output tensor:\\n\", output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example 2x8 input on GPU\n",
    "# Row 0 has some '1's in both block A ([0..3]) and block B ([4..7])\n",
    "# Row 1 also has some '1's in both blocks\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 1, 0, 0, 1, 1, 0],  # row 0\n",
    "        [1, 0, 0, 1, 1, 0, 0, 1]   # row 1\n",
    "    ],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# 1) Find all column indices where row0 == 1 and row1 == 1\n",
    "idx0 = (input_tensor[0] == 1).nonzero(as_tuple=False).flatten()\n",
    "idx1 = (input_tensor[1] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "# 2) Generate the Cartesian product of idx0, idx1 (no loops)\n",
    "grid0, grid1 = torch.meshgrid(idx0, idx1, indexing='ij')\n",
    "combos = torch.stack([grid0.flatten(), grid1.flatten()], dim=-1)  # shape [N0*N1, 2]\n",
    "\n",
    "# 3) Mask out cross-block combos\n",
    "#    Keep only pairs where both columns are in [0..3] or both in [4..7]\n",
    "mask = (\n",
    "    ((combos[:, 0] < 4) & (combos[:, 1] < 4)) |    # both in block A\n",
    "    ((combos[:, 0] >= 4) & (combos[:, 1] >= 4))    # both in block B\n",
    ")\n",
    "combos = combos[mask]\n",
    "\n",
    "# 4) Scatter into the final tensor of shape [num_combos, 2, 8]\n",
    "num_combos = combos.size(0)\n",
    "output_tensor = torch.zeros((num_combos, 2, 8), device='cuda')\n",
    "batch_idx = torch.arange(num_combos, device='cuda')\n",
    "\n",
    "# row=0 => combos[:,0], row=1 => combos[:,1]\n",
    "output_tensor[batch_idx, 0, combos[:, 0]] = 1\n",
    "output_tensor[batch_idx, 1, combos[:, 1]] = 1\n",
    "\n",
    "print(\"Output tensor shape:\", output_tensor.shape)   # e.g. [num_combos, 2, 8]\n",
    "print(\"Example combination:\\n\", output_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def build_block_combos(row0_indices, row1_indices, offset, total_cols, device):\n",
    "    \"\"\"\n",
    "    Build all combinations for a single block (of width 4).\n",
    "    - `row0_indices`: columns (0..3 relative) in this block where row0 == 1\n",
    "    - `row1_indices`: columns (0..3 relative) in this block where row1 == 1\n",
    "    - `offset`: 0 for block A, 4 for block B\n",
    "    - `total_cols`: 8 in a 2x8 tensor\n",
    "    - Returns: [N, 2, total_cols], each slice has one '1' in row0 & row1 (same block),\n",
    "      or single-sided if `row1_indices` is empty.\n",
    "    \"\"\"\n",
    "    # If row1 has no valid columns in this block => single-sided combos for row0\n",
    "    if row1_indices.numel() == 0:\n",
    "        num_combos = row0_indices.size(0)\n",
    "        out = torch.zeros((num_combos, 2, total_cols), device=device)\n",
    "        batch = torch.arange(num_combos, device=device)\n",
    "        # Place '1' for row0 at each valid column + offset\n",
    "        out[batch, 0, row0_indices + offset] = 1\n",
    "        return out\n",
    "\n",
    "    # Otherwise, we do the normal pairwise meshgrid for row0 & row1 columns in this block\n",
    "    grid0, grid1 = torch.meshgrid(row0_indices, row1_indices, indexing='ij')\n",
    "    combos = torch.stack([grid0.flatten(), grid1.flatten()], dim=-1)  # [N0*N1, 2]\n",
    "\n",
    "    num_combos = combos.size(0)\n",
    "    out = torch.zeros((num_combos, 2, total_cols), device=device)\n",
    "    batch = torch.arange(num_combos, device=device)\n",
    "\n",
    "    # Scatter '1's: row0 => combos[:,0] + offset, row1 => combos[:,1] + offset\n",
    "    out[batch, 0, combos[:, 0] + offset] = 1\n",
    "    out[batch, 1, combos[:, 1] + offset] = 1\n",
    "    return out\n",
    "\n",
    "def build_no_cross_combos_2x8(input_2x8: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build all valid combinations for a 2x8 tensor, ensuring no cross-block (A->B or B->A).\n",
    "    - Block A: columns [0..3]\n",
    "    - Block B: columns [4..7]\n",
    "    - If row1 has no valid columns in a block, produce single-sided combos for row0 in that block.\n",
    "    - Returns [N, 2, 8].\n",
    "    \"\"\"\n",
    "    device = input_2x8.device\n",
    "    total_cols = input_2x8.shape[1]  # 8\n",
    "\n",
    "    # --- Gather block A indices ---\n",
    "    row0_A = (input_2x8[0, :4] == 1).nonzero(as_tuple=False).flatten()  # col indices in [0..3]\n",
    "    row1_A = (input_2x8[1, :4] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # --- Gather block B indices ---\n",
    "    row0_B = (input_2x8[0, 4:] == 1).nonzero(as_tuple=False).flatten()  # col indices in [0..3] for that slice\n",
    "    row1_B = (input_2x8[1, 4:] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # Build combos for block A (offset=0)\n",
    "    combosA = build_block_combos(row0_A, row1_A, offset=0, total_cols=total_cols, device=device)\n",
    "    # Build combos for block B (offset=4)\n",
    "    combosB = build_block_combos(row0_B, row1_B, offset=4, total_cols=total_cols, device=device)\n",
    "\n",
    "    # Concatenate along the batch dimension\n",
    "    return torch.cat([combosA, combosB], dim=0)\n",
    "\n",
    "# Example tensor on GPU\n",
    "input_tensor = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 1, 0, 0, 1, 1, 0],  # row 0\n",
    "        [1, 0, 0, 1, 0, 0, 0, 0]   # row 1\n",
    "    ],\n",
    "    device='cuda'\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# USAGE EXAMPLE\n",
    "# --------------------\n",
    "final_output = build_no_cross_combos_2x8(input_tensor)\n",
    "print(\"Final output shape:\", final_output.shape)\n",
    "for i in range(len(final_output)):\n",
    "    print(f\"Combination {i}:\\n\", final_output[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_no_cross_combos_batched(batched_2x8: torch.Tensor):\n",
    "    \"\"\"\n",
    "    For each [i, :, :] slice in a [B, 2, 8] tensor, build the no-cross-block combos\n",
    "    and collect them in a list. Each element of the list has shape [N_i, 2, 8].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    B = batched_2x8.shape[0]\n",
    "    return [build_no_cross_combos_2x8(batched_2x8[i]) for i in range(B)]\n",
    "    # for i in range(B):\n",
    "    #     slice_2x8 = batched_2x8[i]       # shape [2,8]\n",
    "    #     combos_i = build_no_cross_combos_2x8(slice_2x8)\n",
    "    #     results.append(combos_i)\n",
    "    # return results\n",
    "\n",
    "# Example usage:\n",
    "input_batched = torch.randint(0, 2, size=(100, 2, 8), device='cuda')  # random 0/1\n",
    "all_combos = build_no_cross_combos_batched(input_batched)\n",
    "\n",
    "# 'all_combos' is a Python list of length 100\n",
    "# each item has shape [N_i, 2, 8], where N_i can differ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import torch\n",
    "import numpy as np \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def find_subsets_with_sum(values, r, target_sum):\n",
    "    subsets = []\n",
    "    \n",
    "    for subset in combinations(values, r):\n",
    "        if sum(subset) == target_sum:\n",
    "            subsets.append(subset)\n",
    "            \n",
    "    return subsets\n",
    "\n",
    "\n",
    "# hand = '3'\n",
    "\n",
    "\n",
    "def look_up_t(hand, ranks):\n",
    "\n",
    "    # ranks_digit = [np.uint8(i+1) for i, count in enumerate([int(ranks[i]) for i in range(10)]) for _ in range(count)] \n",
    "    ranks_digit = [i+1 for i, count in enumerate([int(ranks[i]) for i in range(10)]) for _ in range(count)] \n",
    "    # import pdb; pdb.set_trace()\n",
    "    ranks_subsets = [find_subsets_with_sum(ranks_digit,r,11-int(hand)) for r in range(1, 8)]\n",
    "    # import pdb; pdb.set_trace()\n",
    "    pools = []\n",
    "    hist = []\n",
    "    for item in ranks_subsets:\n",
    "        # import pdb; pdb.set_trace()\n",
    "        if len(item)>0:\n",
    "            for sub_item in item:\n",
    "                # sorted(sub_item)\n",
    "                # print(sub_item)\n",
    "                # import pdb; pdb.set_trace()\n",
    "                sub_item = sorted(sub_item)\n",
    "                if sub_item not in hist:\n",
    "                    hist.append(sub_item)\n",
    "                    # print(sub_item)\n",
    "                    # pools.append({key: np.uint8(value) for key, value in Counter(sub_item).items()})\n",
    "                    pools.append(dict(Counter(sub_item)))\n",
    "                    # import pdb; pdb.set_trace()\n",
    "    return pools \n",
    "\n",
    "\n",
    "HANDS = [i for i in range(1,11)]\n",
    "RANKS = [f'{i1}{i2}{i3}{i4}{i5}{i6}{i7}{i8}{i9}{i10}' for i1,i2,i3,i4,i5,i6,i7,i8,i9,i10 in itertools.product(range(5), repeat=10)]\n",
    "# RANKS = RANKS[:16*5]\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "full_res = {\n",
    "    hand: look_up_t(hand, '4444444444')\n",
    "    for hand in HANDS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Boolean tensor\n",
    "x = torch.tensor([0, 1, 1, 0], dtype=torch.uint8)\n",
    "print(\"x =\", x)                 # tensor([ True, False,  True, False])\n",
    "\n",
    "y = torch.bitwise_not(x)\n",
    "# same as y = ~x\n",
    "print(\"bitwise_not(x) =\", y)    # tensor([False,  True, False,  True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# 1. Create the array of \"values\" for each index i in [0..39].\n",
    "#    value(i) = ceil(1 + i/4)\n",
    "vals = torch.tensor([math.floor(1 + i/4) for i in range(40)], dtype=torch.int64)\n",
    "\n",
    "# We want subsets that sum exactly to 11.\n",
    "TARGET_SUM = 11\n",
    "N = 40\n",
    "\n",
    "# 2. Create a DP table: dp[i, s] is True if we can form sum 's' using first i items.\n",
    "#    We'll make dp have shape (N+1, TARGET_SUM+1).\n",
    "dp = torch.zeros((N + 1, TARGET_SUM + 1), dtype=torch.bool)\n",
    "\n",
    "# Base case: dp[0, 0] = True (subset of size 0 has sum 0).\n",
    "dp[0, 0] = True\n",
    "\n",
    "# 3. Fill in the DP table.\n",
    "#    i goes from 1..N, s goes from 0..TARGET_SUM\n",
    "for i in range(1, N + 1):\n",
    "    v = vals[i - 1].item()  # the value of the (i-1)th element\n",
    "    for s in range(TARGET_SUM + 1):\n",
    "        # Option 1: Skip this item\n",
    "        if dp[i - 1, s]:\n",
    "            dp[i, s] = True\n",
    "        # Option 2: Take this item (if s >= v)\n",
    "        elif s >= v and dp[i - 1, s - v]:\n",
    "            dp[i, s] = True\n",
    "\n",
    "# Check if there's at least one solution\n",
    "if not dp[N, TARGET_SUM]:\n",
    "    print(\"No subsets sum to 11.\")\n",
    "else:\n",
    "    print(\"There is at least one subset summing to 11. Now finding all...\")\n",
    "\n",
    "    # 4. Backtracking to retrieve ALL subsets that yield sum = 11.\n",
    "    #    This can be large, so be cautious about memory/time usage!\n",
    "\n",
    "    def backtrack(i, s):\n",
    "        \"\"\"\n",
    "        Yields *all* subsets (as lists of indices) using the first i items\n",
    "        that sum to s.\n",
    "        \"\"\"\n",
    "        # If we've hit sum=0, we have a valid subset (empty at this stage).\n",
    "        if s == 0:\n",
    "            yield []\n",
    "            return\n",
    "\n",
    "        # If we've run out of items, no solution from here on.\n",
    "        if i == 0:\n",
    "            return\n",
    "\n",
    "        # Check possibility #1: we did NOT use the i-1 th item.\n",
    "        if dp[i - 1, s]:\n",
    "            for subset in backtrack(i - 1, s):\n",
    "                yield subset\n",
    "\n",
    "        # Check possibility #2: we used the i-1 th item (if feasible).\n",
    "        v = vals[i - 1].item()\n",
    "        if s >= v and dp[i - 1, s - v]:\n",
    "            for subset in backtrack(i - 1, s - v):\n",
    "                yield subset + [i - 1]\n",
    "\n",
    "    all_solutions = []\n",
    "    for subset_indices in backtrack(N, TARGET_SUM):\n",
    "        # Build a 0/1 tensor of length 40\n",
    "        solution_tensor = torch.zeros(N, dtype=torch.uint8)\n",
    "        for idx in subset_indices:\n",
    "            solution_tensor[idx] = 1\n",
    "        all_solutions.append(solution_tensor)\n",
    "\n",
    "all_solutions_tensor = torch.stack(all_solutions, dim=0)\n",
    "idx = {i:(all_solutions_tensor[:, i]).nonzero(as_tuple=True)[0] for i in range(40)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x = torch.zeros(40, dtype=torch.uint8)\n",
    "indices = [1]\n",
    "x[indices] = 1\n",
    "x = x.unsqueeze(0)\n",
    "all_solutions_tensor[idx[39]]\n",
    "mask_A = ((all_solutions_tensor[idx[39]] & x) == all_solutions_tensor[idx[39]]).all(dim=1)\n",
    "mask_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_solutions_tensor[idx[39]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_block_combos(slice_2x52, col_start, col_end):\n",
    "    \"\"\"\n",
    "    Build combos within columns [col_start:col_end],\n",
    "    for a 2x52 slice, using our previous block logic:\n",
    "     - row0 = slice_2x52[0], row1 = slice_2x52[1].\n",
    "     - single-sided if row1 has no 1s in that block.\n",
    "    Returns a [N, 2, 52] tensor.\n",
    "    \"\"\"\n",
    "    device = slice_2x52.device\n",
    "    # Identify columns where row0/row1 == 1 in [col_start..col_end)\n",
    "    sub_width = col_end - col_start  # usually 4\n",
    "    row0_indices = (slice_2x52[0, col_start:col_end] == 1).nonzero(as_tuple=False).flatten()\n",
    "    row1_indices = (slice_2x52[1, col_start:col_end] == 1).nonzero(as_tuple=False).flatten()\n",
    "\n",
    "    # If row1 has no valid columns => single-sided combos\n",
    "    if row1_indices.numel() == 0:\n",
    "        num_combos = row0_indices.size(0)\n",
    "        out = torch.zeros((num_combos, 2, 52), device=device)\n",
    "        if num_combos > 0:\n",
    "            out_idx = torch.arange(num_combos, device=device)\n",
    "            out[out_idx, 0, row0_indices + col_start] = 1\n",
    "        return out\n",
    "\n",
    "    # Otherwise, pairwise combos via meshgrid\n",
    "    grid0, grid1 = torch.meshgrid(row0_indices, row1_indices, indexing='ij')\n",
    "    combos = torch.stack([grid0.flatten(), grid1.flatten()], dim=-1)  # shape [N0*N1, 2]\n",
    "    num_c = combos.size(0)\n",
    "\n",
    "    out = torch.zeros((num_c, 2, 52), device=device)\n",
    "    out_idx = torch.arange(num_c, device=device)\n",
    "    out[out_idx, 0, combos[:, 0] + col_start] = 1\n",
    "    out[out_idx, 1, combos[:, 1] + col_start] = 1\n",
    "    return out\n",
    "\n",
    "def build_no_cross_2x52_in_last_8(slice_2x52):\n",
    "    \"\"\"\n",
    "    For a 2x52 slice, build combos in the last 8 columns ([44..52)):\n",
    "     - Split into block A = [44..48), block B = [48..52)\n",
    "     - No cross-block needed here, each block is handled independently\n",
    "     - Return combosA, combosB (both are [N, 2, 52] but possibly different N).\n",
    "    \"\"\"\n",
    "    # columns [44..48) => block A\n",
    "    combosA = build_block_combos(slice_2x52, col_start=44, col_end=48)\n",
    "    # columns [48..52) => block B\n",
    "    combosB = build_block_combos(slice_2x52, col_start=48, col_end=52)\n",
    "    return combosA, combosB\n",
    "\n",
    "def build_single_sided_for_minus12_to_minus8(slice_2x52):\n",
    "    \"\"\"\n",
    "    Look at columns [-12..-8) => [40..44), but only for row0 in our slice_2x52.\n",
    "    For each '1' in row0[40..44), build a new [2, 52] combo:\n",
    "      - row0 has '1' at that column (single-sided).\n",
    "      - row1 is copied from slice_2x52[1, :], except columns [44..52) forced to 0.\n",
    "    Returns shape [K, 2, 52], one slice per '1' in row0[40..44).\n",
    "    \"\"\"\n",
    "    device = slice_2x52.device\n",
    "    row0_indices = (slice_2x52[0, 40:44] == 1).nonzero(as_tuple=False).flatten()\n",
    "    num_combos = row0_indices.size(0)\n",
    "\n",
    "    # Prepare the output\n",
    "    out = torch.zeros((num_combos, 2, 52), device=device)\n",
    "\n",
    "    if num_combos > 0:\n",
    "        # 1) Copy row1 from the original slice for *each* combo\n",
    "        out[:, 1, :] = slice_2x52[1]\n",
    "\n",
    "        # 2) Force row1 columns [44..52) = 0\n",
    "        out[:, 1, 44:52] = 0\n",
    "\n",
    "        # 3) Place '1' in row0 at the chosen columns [40..44)\n",
    "        out_idx = torch.arange(num_combos, device=device)\n",
    "        out[out_idx, 0, row0_indices + 40] = 1\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_for_0x39(slice_2x52):\n",
    "    \"\"\"\n",
    "    Look at columns [-12..-8) => [40..44), but only for row0 in our slice_2x52.\n",
    "    For each '1' in row0[40..44), build a new [2, 52] combo:\n",
    "      - row0 has '1' at that column (single-sided).\n",
    "      - row1 is copied from slice_2x52[1, :], except columns [44..52) forced to 0.\n",
    "    Returns shape [K, 2, 52], one slice per '1' in row0[40..44).\n",
    "    \"\"\"\n",
    "    device = slice_2x52.device\n",
    "    row0_indices = (slice_2x52[0,:40] == 1).nonzero(as_tuple=False).flatten()\n",
    "    # print(row0_indices)\n",
    "    # num_combos = row0_indices.size(0)\n",
    "    for index in row0_indices:\n",
    "        # print(index)\n",
    "        candidates = all_solutions_tensor[idx[index.item()]]\n",
    "        # print(candidates)\n",
    "        mask_A = ((candidates & slice_2x52[0,:40]) == candidates).all(dim=1)\n",
    "\n",
    "        \n",
    "        # print(mask_A)\n",
    "    # print(slice_2x52)\n",
    "    # # Prepare the output\n",
    "    # out = torch.zeros((num_combos, 2, 52), device=device)\n",
    "\n",
    "    # if num_combos > 0:\n",
    "    #     # 1) Copy row1 from the original slice for *each* combo\n",
    "    #     out[:, 1, :] = slice_2x52[1]\n",
    "\n",
    "    #     # 2) Force row1 columns [44..52) = 0\n",
    "    #     out[:, 1, 44:52] = 0\n",
    "\n",
    "    #     # 3) Place '1' in row0 at the chosen columns [40..44)\n",
    "    #     out_idx = torch.arange(num_combos, device=device)\n",
    "    #     out[out_idx, 0, row0_indices + 40] = 1\n",
    "\n",
    "    return mask_A\n",
    "\n",
    "def process_13x52(input_13x52, p):\n",
    "    \"\"\"\n",
    "    Main routine:\n",
    "      1) Pick which row is 'top' (row0 vs row1) based on p, and the 'bottom' row is row2.\n",
    "      2) Form a 2x52 slice for those two rows.\n",
    "      3) Build combosA, combosB in the last 8 columns.\n",
    "      4) Build single-sided combos for columns [40..44) in top row.\n",
    "      5) Return them all (not concatenated).\n",
    "    \"\"\"\n",
    "    device = input_13x52.device\n",
    "\n",
    "    slice_2x52 = input_13x52[[p, 2]]\n",
    "\n",
    "    # Step 3: combosA, combosB in the last 8 columns\n",
    "    combosA, combosB = build_no_cross_2x52_in_last_8(slice_2x52)\n",
    "\n",
    "    # Step 4: single-sided combos for columns [40..44) in top row\n",
    "    single_sided = build_single_sided_for_minus12_to_minus8(slice_2x52)\n",
    "    # print(build_for_0x39(slice_2x52))\n",
    "    # Step 5: Return them separately (not concatenated).\n",
    "    maks_A = build_for_0x39(slice_2x52)\n",
    "    return mask_A\n",
    "    # return combosA, combosB, single_sided\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def build_block_for_index(index, row0, all_solutions_tensor, idx):\n",
    "    \"\"\"\n",
    "    For a single 'index':\n",
    "      1) Gather the candidate rows: candidates = all_solutions_tensor[idx[index]]\n",
    "      2) Build a mask for 'subset' check: (candidate & row0) == candidate\n",
    "      3) If we have valid rows, stack them with row0 into shape (num_valid, 2, 40)\n",
    "      4) Otherwise, return shape (1, 2, 40) with row0 on top and zeros on the bottom.\n",
    "\n",
    "    Returns:\n",
    "      A tensor of shape (X, 2, 40), where X >= 1.\n",
    "    \"\"\"\n",
    "    # 1) Gather candidate rows\n",
    "    candidates = all_solutions_tensor[idx[index.item()]]  # shape (M, 40)\n",
    "\n",
    "    # 2) Vectorized subset check (no per-row Python loop):\n",
    "    #    We want (candidates <= row0) in a 0/1 sense => (c & row0) == c\n",
    "    mask_A = ((candidates & row0) == candidates).all(dim=1)  # shape (M,)\n",
    "\n",
    "    num_valid = mask_A.sum().item()\n",
    "    if num_valid > 0:\n",
    "        # 3) Some valid rows => stack them with row0\n",
    "        valid_rows = candidates[mask_A]  # shape (num_valid, 40)\n",
    "        # Expand row0 to match num_valid rows, shape (num_valid, 40)\n",
    "        row0_expanded = row0.unsqueeze(0).expand(num_valid, -1)\n",
    "        # Stack along a new dim => (num_valid, 2, 40)\n",
    "        block = torch.stack([row0_expanded, valid_rows], dim=1)\n",
    "    else:\n",
    "        # 4) No valid candidates => return (1, 2, 40) with row0 + zeros\n",
    "        block = torch.zeros((1, 2, 40), dtype=row0.dtype, device=row0.device)\n",
    "        block[0, :] = row0\n",
    "\n",
    "    return block\n",
    "\n",
    "def process_all_indices(row0_indices, row0, all_solutions_tensor, idx):\n",
    "    \"\"\"\n",
    "    For each 'index' in row0_indices, build a block with build_block_for_index().\n",
    "    Then concatenate blocks along dim=0 into one big tensor (X, 2, 40).\n",
    "    If row0_indices is empty, return (0, 2, 40).\n",
    "    \"\"\"\n",
    "    blocks = [\n",
    "        build_block_for_index(ind, row0, all_solutions_tensor, idx) \n",
    "        for ind in row0_indices\n",
    "    ]\n",
    "    if blocks:\n",
    "        return torch.cat(blocks, dim=0)\n",
    "    else:\n",
    "        return torch.empty((0, 2, 40), dtype=row0.dtype, device=row0.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Suppose we have a 13x52 tensor on GPU\n",
    "# We'll just make a random 0/1 example here on CPU for demonstration.\n",
    "input_13x52 = torch.randint(0, 2, (13, 52))\n",
    "\n",
    "p = 0  # or 1\n",
    "combosA, combosB, single_sided, onex40 = process_13x52(input_13x52, p)\n",
    "mask_A = process_13x52(input_13x52, p)\n",
    "# print(\"CombosA shape:\", combosA.shape)\n",
    "# print(\"CombosB shape:\", combosB.shape)\n",
    "# print(\"Single-sided shape:\", single_sided.shape)\n",
    "\n",
    "# combosA, combosB, single_sided are each [N, 2, 52] but can have different N.\n",
    "# They are NOT concatenated here; you can later do so if desired.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Mock data\n",
    "    slice_2x52 = torch.randint(0, 2, (2, 52), dtype=torch.uint8)\n",
    "    # Suppose row0 is the first row's first 40 columns\n",
    "    row0 = slice_2x52[0, :40]\n",
    "\n",
    "    # row0_indices: positions where row0 == 1\n",
    "    row0_indices = (row0 == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Now build the final result\n",
    "    final_result = process_all_indices(row0_indices, row0, all_solutions_tensor, idx)\n",
    "\n",
    "    print(\"slice_2x52:\\n\", slice_2x52)\n",
    "    print(\"row0:\", row0)\n",
    "    print(\"row0_indices:\", row0_indices.tolist())\n",
    "    print(\"final_result shape:\", final_result.shape)\n",
    "    if final_result.size(0) > 0:\n",
    "        print(\"first block:\\n\", final_result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros((0, 2, 52))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "deck = list(range(52))\n",
    "random.shuffle(deck)\n",
    "print(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = torch.tensor(range(52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck = torch.arange(52,dtype=torch.uint8, device=device)\n",
    "while True:\n",
    "    deck = deck[torch.randperm(deck.size(0))]\n",
    "    if not any([x in [40, 41, 42, 43] for x in deck[4:8] ]):\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create sparse tensors\n",
    "indices1 = torch.tensor([[0, 1, 1], [2, 0, 2]])  # Coordinates of non-zero elements\n",
    "values1 = torch.tensor([3.0, 4.0, 5.0])          # Non-zero values\n",
    "sparse_tensor1 = torch.sparse_coo_tensor(indices1, values1, (3, 3))\n",
    "\n",
    "indices2 = torch.tensor([[0, 1], [2, 2]])\n",
    "values2 = torch.tensor([1.0, 2.0])\n",
    "sparse_tensor2 = torch.sparse_coo_tensor(indices2, values2, (3, 3))\n",
    "\n",
    "# Subtract the sparse tensors\n",
    "result = sparse_tensor1 - sparse_tensor2\n",
    "\n",
    "# Convert to dense for visualization (optional)\n",
    "print(\"Result (dense):\")\n",
    "print(result.to_dense())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example dictionary\n",
    "d = {0: 10, 1: 20, 2: 30, 3: 40, 4: 50}\n",
    "\n",
    "# Example tensor of indices\n",
    "tensor_a = torch.tensor([1, 3, 0, 4, 2])  # Indices from the dictionary\n",
    "\n",
    "# Convert dictionary values to a tensor (lookup table)\n",
    "max_index = max(d.keys()) + 1  # Ensure all indices fit\n",
    "lookup_table = torch.zeros(max_index, dtype=torch.long)\n",
    "for k, v in d.items():\n",
    "    lookup_table[k] = v\n",
    "\n",
    "# Efficient lookup using tensor indexing\n",
    "result = lookup_table[tensor_a]\n",
    "\n",
    "print(result)  # Output: tensor([20, 40, 10, 50, 30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the indices of nonzero elements (COO format)\n",
    "indices = torch.tensor([[0, 1, 2],  # Row indices\n",
    "                        [1, 0, 2]])  # Column indices\n",
    "\n",
    "# Define the values at those indices\n",
    "values = torch.tensor([1, 2, 3])  # Values at (0,1), (1,0), and (2,2)\n",
    "\n",
    "# Define the shape of the sparse tensor\n",
    "size = (3, 3)\n",
    "\n",
    "# Create the sparse tensor\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices, values, size)\n",
    "\n",
    "print(\"Sparse Tensor:\")\n",
    "print(sparse_tensor)\n",
    "\n",
    "# Extracting row 0 indices\n",
    "row_0_mask = indices[0] == 0  # Select elements from row 0\n",
    "row_0_indices = indices[1][row_0_mask]  # Get column indices where row = 0\n",
    "\n",
    "print(\"Row 0 Indices:\", row_0_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sparse_tensor+sparse_tensor).to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_map = {0: 20, 1: 100}  # Assign custom sizes\n",
    "df[\"marker_size\"] = df[\"binary_column\"].map(size_map)\n",
    "\n",
    "sns.scatterplot(data=df, x=\"A\", y=\"B\", hue=\"category_column\", size=\"marker_size\", sizes=(10, 200))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the indices of non-zero elements\n",
    "indices1 = torch.tensor([[0, 1, 2], [2, 0, 1]])  # COO format: (2 x nnz)\n",
    "values1 = torch.tensor([1.0, 2.0, 3.0])         # Values of non-zero elements\n",
    "sparse_tensor1 = torch.sparse_coo_tensor(indices1, values1, size=(3, 3))\n",
    "\n",
    "indices2 = torch.tensor([[0, 1, 2], [1, 2, 0]])  # COO format: (2 x nnz)\n",
    "values2 = torch.tensor([4.0, 5.0, 6.0])         # Values of non-zero elements\n",
    "sparse_tensor2 = torch.sparse_coo_tensor(indices2, values2, size=(3, 3))\n",
    "\n",
    "# Ensure the sparse tensors are coalesced (recommended for operations)\n",
    "sparse_tensor1 = sparse_tensor1.coalesce()\n",
    "sparse_tensor2 = sparse_tensor2.coalesce()\n",
    "# Perform matrix multiplication\n",
    "result = torch.sparse.mm(sparse_tensor1, sparse_tensor2)\n",
    "\n",
    "# Convert the result to a dense tensor for easier inspection (optional)\n",
    "result_dense = result.to_dense()\n",
    "print(result_dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a sparse tensor with duplicate indices\n",
    "indices = torch.tensor([[0, 1, 0], [2, 0, 2]])  # Duplicate index (0, 2)\n",
    "values = torch.tensor([1.0, 2.0, 3.0])          # Values for the indices\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices, values, size=(3, 3))\n",
    "\n",
    "print(\"Before coalescing:\")\n",
    "print(\"Indices:\\n\", sparse_tensor.indices())\n",
    "print(\"Values:\\n\", sparse_tensor.values())\n",
    "\n",
    "# Coalesce the sparse tensor\n",
    "coalesced_tensor = sparse_tensor.coalesce()\n",
    "\n",
    "print(\"\\nAfter coalescing:\")\n",
    "print(\"Indices:\\n\", coalesced_tensor.indices())\n",
    "print(\"Values:\\n\", coalesced_tensor.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create the tensors\n",
    "tensor1 = torch.rand(8, 2, 4)  # Shape: (8, 2, 40)\n",
    "tensor2 = torch.randn(4, 7)    # Shape: (40, 27)\n",
    "\n",
    "# Perform batch matrix multiplication\n",
    "result = torch.matmul(tensor1, tensor2)  # Shape: (8, 2, 27)\n",
    "\n",
    "# Alternatively, you can use the @ operator\n",
    "# result = tensor1 @ tensor2\n",
    "\n",
    "print(\"Shape of tensor1:\", tensor1.shape)\n",
    "print(\"Shape of tensor2:\", tensor2.shape)\n",
    "print(\"Shape of result:\", result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of shape (N, 2, 40)\n",
    "N = 3\n",
    "tensor = torch.arange(N * 2 * 4).reshape(N, 2, 4)  # Example tensor\n",
    "\n",
    "# Reshape to (2*N, 40)\n",
    "flattened_tensor = tensor.reshape(2 * N, 4)\n",
    "\n",
    "print(\"Original tensor shape:\", tensor.shape)\n",
    "print(\"Flattened tensor shape:\", flattened_tensor.shape)\n",
    "# Reshape back to (N, 2, 40)\n",
    "restored_tensor = flattened_tensor.reshape(N, 2, 4)\n",
    "\n",
    "print(\"Restored tensor shape:\", restored_tensor.shape)\n",
    "\n",
    "# Check if the restored tensor matches the original\n",
    "print(\"Are the tensors equal?\", torch.equal(tensor, restored_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "slice_2x40 = torch.randn(8, 2, 40)  # Shape: (8, 2, 40)\n",
    "all_solutions_tensor = torch.randn(40, 27)  # Shape: (40, 27)\n",
    "\n",
    "# Add a batch dimension to all_solutions_tensor\n",
    "all_solutions_tensor_3d = all_solutions_tensor.unsqueeze(0)  # Shape: (1, 40, 27)\n",
    "\n",
    "# Expand the batch dimension to match slice_2x40\n",
    "all_solutions_tensor_3d = all_solutions_tensor_3d.expand(slice_2x40.size(0), -1, -1)  # Shape: (8, 40, 27)\n",
    "\n",
    "# Perform batch matrix multiplication\n",
    "result = torch.bmm(slice_2x40, all_solutions_tensor_3d)  # Shape: (8, 2, 27)\n",
    "\n",
    "print(\"Shape of result:\", result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Example neural network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create a tensor of type torch.uint8 (Byte)\n",
    "uint8_tensor = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.uint8)\n",
    "\n",
    "# Step 1: Convert to floating-point type\n",
    "float_tensor = uint8_tensor.to(torch.float32)  # or use .float()\n",
    "\n",
    "# Step 2: Normalize (if necessary)\n",
    "# For example, if the tensor represents pixel values in [0, 255], normalize to [0, 1]\n",
    "float_tensor = float_tensor / 255.0\n",
    "\n",
    "# Step 3: Pass the tensor to the neural network\n",
    "model = SimpleNet()\n",
    "output = model(float_tensor.unsqueeze(0))  # Add batch dimension if needed\n",
    "\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100  # Example value for N\n",
    "zero_tensor = torch.zeros(8, N, 40)\n",
    "print(zero_tensor.shape)  # Output: torch.Size([8, N, 40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two tensors of shape [2, 3]\n",
    "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "tensor2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Element-wise multiplication using the * operator\n",
    "result = tensor1 * tensor2\n",
    "\n",
    "# Alternatively, use torch.mul\n",
    "# result = torch.mul(tensor1, tensor2)\n",
    "\n",
    "print(\"Tensor 1:\\n\", tensor1)\n",
    "print(\"Tensor 2:\\n\", tensor2)\n",
    "print(\"Element-wise multiplication result:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example data\n",
    "cc = torch.randint(0, 2, (3, 7), dtype=torch.bool, device=\"cuda\")  # 8 x 2764 mask\n",
    "Np = torch.tensor([True, False, True], device=\"cuda\")  # 8-length mask\n",
    "\n",
    "# Step 1: Select only the rows where Np is True\n",
    "cc_selected = cc[Np]  # Shape: (num_selected_rows, 2764)\n",
    "\n",
    "# Step 2: Reshape to (2764 * num_selected_rows,)\n",
    "cc_reshaped = cc_selected.reshape(-1)\n",
    "\n",
    "print(cc_selected.shape)  # Expected: (num_selected_rows, 2764)\n",
    "print(cc_reshaped.shape)  # Expected: (2764 * num_selected_rows,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "zero_tensor = torch.zeros(6, 4)  # Shape (6,4)\n",
    "result_hand = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])\n",
    "result = torch.tensor([[2, 2, 2, 2], [3, 3, 3, 3], [4, 4, 4, 4]])\n",
    "\n",
    "# Apply operation\n",
    "zero_tensor[::2, :] = result_hand * result\n",
    "\n",
    "print(zero_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor (14, 40)\n",
    "x = torch.randn(4, 3)\n",
    "\n",
    "# Grouping tensor indicating sum in pairs (shape [7,])\n",
    "grouping = torch.tensor([2, 2])\n",
    "\n",
    "# Create an index tensor that will specify where to add the rows\n",
    "index = torch.repeat_interleave(torch.arange(grouping.size(0)), grouping)  # Shape: [14]\n",
    "index = index.view(-1, 1).expand(-1, x.size(1))  # Expand to match x's columns\n",
    "\n",
    "# Create the output tensor of shape [7, 40]\n",
    "output = torch.zeros(grouping.size(0), x.size(1))\n",
    "\n",
    "# Sum rows using scatter_add\n",
    "output.scatter_add_(0, index, x)\n",
    "\n",
    "print(output.shape)  # Should output torch.Size([7, 40])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [-0.5816,  1.4321,  1.0590][ 0.2138, -1.0521, -0.0380]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-0.5816+0.2138"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor\n",
    "I = torch.tensor([[1, 1, 1, 1, 0], [2, 1, 0, 1, 1]])\n",
    "\n",
    "# Example tensors tk, tq, tj, tn, tn2\n",
    "tk = torch.arange(10)  # Example tensor\n",
    "tq = torch.arange(10, 20)\n",
    "tj = torch.arange(20, 30)\n",
    "tn = torch.arange(30, 40)\n",
    "tn2 = torch.arange(40, 50)\n",
    "\n",
    "# Store the tensors in a list for easy indexing\n",
    "tensor_list = [tk, tq, tj, tn, tn2]\n",
    "\n",
    "# Flatten I and use advanced indexing\n",
    "I_flat = I.flatten()\n",
    "Z = torch.cat([tensor_list[idx][i].unsqueeze(0) for i, idx in enumerate(I_flat)])\n",
    "\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define tensors (each of shape (2,1))\n",
    "tk = torch.tensor([[10], [20]])  # Shape (2,1)\n",
    "tq = torch.tensor([[30], [40]])\n",
    "tj = torch.tensor([[50], [60]])\n",
    "tn = torch.tensor([[70], [80]])\n",
    "tn2 = torch.tensor([[90], [100]])\n",
    "\n",
    "# Stack tensors to form shape (5, 2, 1)\n",
    "T = torch.stack([tk, tq, tj, tn, tn2])  # Shape: (5, 2, 1)\n",
    "\n",
    "# Define index tensor\n",
    "I = torch.tensor([\n",
    "    [1, 1, 1, 1, 0], \n",
    "    [2, 1, 0, 1, 1]\n",
    "])\n",
    "\n",
    "# Expand column indices to match dimensions\n",
    "rows = I  # Select row indices\n",
    "cols = torch.zeros_like(I)  # Since tensors have shape (2,1), column index is always 0\n",
    "\n",
    "# Use advanced indexing to construct Z\n",
    "Z = T[rows, torch.arange(I.shape[0])[:, None], cols]\n",
    "\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "dim = 0 \n",
    "src = torch.rand(size=(4,4,6))\n",
    "index = torch.randint(low=0,high=4,size=(4,6))\n",
    "\n",
    "out = torch.zeros_like(index)\n",
    "\n",
    "for j in range(4):\n",
    "    for k in range(6):\n",
    "\n",
    "        out[j,k] = src[index[j,k],j, k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.ones((3, 5))\n",
    "index = torch.tensor([[1, 0, 0,2, 0],[1, 1, 0, 0, 0]])\n",
    "torch.zeros(2, 5, dtype=src.dtype).scatter_add_(1, index, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Input tensor (4x3)\n",
    "src = torch.tensor([[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 8, 9],\n",
    "                    [10, 11, 12]], dtype=torch.float32)\n",
    "\n",
    "# Index tensor: specifies which rows to sum\n",
    "# Sum the first two rows (indices 0 and 1) and the last two rows (indices 2 and 3)\n",
    "index = torch.tensor([0, 0, 1, 1])  # 0 for first two rows, 1 for last two rows\n",
    "\n",
    "# Target tensor: initialize with zeros\n",
    "# The shape is (2, 3) because we are summing into 2 groups\n",
    "out = torch.zeros(2, 3, dtype=torch.float32)\n",
    "\n",
    "# Perform scatter_add_\n",
    "out.scatter_add_(0, index.unsqueeze(1).expand_as(src), src)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.unsqueeze(1).expand_as(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.ones((2, 5))\n",
    "index = torch.tensor([[1,1,1,1,1],[1,1,1,1,1]])\n",
    "torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
    "# index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n",
    "# torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Pdb) hand_moves\n",
    "# tensor([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0.],\n",
    "#         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "#          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
    "#          0., 0., 0., 0.]], device='cuda:0', dtype=torch.float16)\n",
    "# (Pdb) hand_moves.shape\n",
    "# torch.Size([9, 40])\n",
    "# (Pdb) moves.shape\n",
    "# torch.Size([9, 40])\n",
    "# (Pdb) indices\n",
    "# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "#          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "#         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "#          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "#         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "#          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
    "#         [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
    "#          2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]], device='cuda:0')\n",
    "\n",
    "# (Pdb) hand_check.shape\n",
    "# torch.Size([3, 40])\n",
    "# (Pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "b = a  # b is just a reference to a\n",
    "c = list(a)  # Creates a new list, but elements share the same memory\n",
    "\n",
    "print(hex(id(a)))  # Memory address of list 'a'\n",
    "print(hex(id(b)))  # Same as 'a' because 'b' is just another reference\n",
    "print(hex(id(c)))  # Different address because 'c' is a new list\n",
    "\n",
    "print(hex(id(a[0])))  # Memory address of the first element\n",
    "print(hex(id(c[0])))  # Same address as 'a[0]' because integers are immutable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([0, 2, 1])\n",
    "torch.repeat_interleave(torch.arange(len(z)), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor (3 rows, 4 columns)\n",
    "x = torch.tensor([[1, 2, 3, 4], \n",
    "                  [5, 6, 7, 8], \n",
    "                  [9, 10, 11, 12]])\n",
    "\n",
    "# Permutation indices (new order of rows)\n",
    "indices = torch.tensor([2, 0, 1])  # Moves row 2 → row 0, row 0 → row 1, row 1 → row 2\n",
    "\n",
    "# Apply permutation\n",
    "permuted_x = x[indices]\n",
    "\n",
    "print(permuted_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a 3D tensor of shape (2, 3, 4)\n",
    "xx = torch.arange(2 * 3 * 4).reshape(2, 3, 4)\n",
    "print(\"Original Tensor:\\n\", xx)\n",
    "\n",
    "# Applying indexing\n",
    "result = xx[:, [0, 2], :]\n",
    "\n",
    "print(\"\\nSelected Tensor:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([10, 20, 30])\n",
    "repeats = torch.tensor([0, 3, 1])  # Number of times to repeat each element\n",
    "\n",
    "result = torch.repeat_interleave(x, repeats)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import math\n",
    "\n",
    "\n",
    "# torch.manual_seed(10)\n",
    "RANKS = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K']\n",
    "SUITS = ['♣', '♦', '♥', '♠'] \n",
    "aces_jackes = [True if x in [0,1,2,3,40,41,42,43] else False for x in range(52)]\n",
    "jackes = [True if x in [40,41,42,43] else False for x in range(52)]\n",
    "\n",
    "clubs = [True if x in [4*x for x in range(13)] else False for x in range(52)] \n",
    "\n",
    "\n",
    "# infoset[-1,:] = deck\n",
    "\n",
    "## step A\n",
    "\n",
    "deck = torch.tensor(range(52))\n",
    "def print_deck(deck):\n",
    "\n",
    "    ranks = [math.floor(t/4) for t in deck]\n",
    "    suits = [t.item()%4 for t in deck]\n",
    "    return [f\"{RANKS[rank]}{SUITS[suit]}\" for (rank,suit) in zip(ranks,suits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_deck(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, card in enumerate(print_deck(deck)): print(cnt,card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "for idx,item in enumerate(print_deck(deck)):\n",
    "    # print(item)\n",
    "    # break \n",
    "    if item[-1] == '♣':\n",
    "        ans.append(int(idx/4))\n",
    "    elif item == '10♦':\n",
    "        ans.append(13)        \n",
    "    else:\n",
    "        rank = RANKS.index(item[:-1])\n",
    "        ans.append(14+rank)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = '123'\n",
    "s2 = 'as73123'\n",
    "s1 in s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "\n",
    "# Example function\n",
    "def f(x, y):\n",
    "    return x < y\n",
    "result = f(a[:, None], b[None, :]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "for p in itertools.product([0, 1, 2], repeat=4):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "t_deck = torch.arange(52, device=device)\n",
    "\n",
    "# Indices of all clubs: 0, 4, 8, ..., 48\n",
    "clubs = torch.arange(0, 52, 4, device=device)  # shape [13]\n",
    "\n",
    "# Non-clubs (the remaining 39 cards)\n",
    "non_clubs = t_deck[~torch.isin(t_deck, clubs)]  # shape [39]\n",
    "\n",
    "# Randomly select 7 non-clubs to go with the 13 clubs in the first 20\n",
    "non_clubs_perm = non_clubs[torch.randperm(non_clubs.size(0))]\n",
    "extra_7 = non_clubs_perm[:7]\n",
    "remaining_non_clubs = non_clubs_perm[7:]\n",
    "\n",
    "# Shuffle the 13 clubs and extra 7, then concatenate with the rest\n",
    "first_20 = torch.cat([clubs, extra_7])[torch.randperm(20)]\n",
    "rest = remaining_non_clubs\n",
    "\n",
    "# Final deck\n",
    "final_deck = torch.cat([first_20, rest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_deck(deck):\n",
    "\n",
    "    ranks = [math.floor(t/4) for t in deck]\n",
    "    suits = [t.item()%4 for t in deck]\n",
    "    return [f\"{RANKS[rank]}{SUITS[suit]}\" for (rank,suit) in zip(ranks,suits)]\n",
    "\n",
    "print_deck(final_deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Suppose t_tot is a 3D tensor. For this example, we'll create one.\n",
    "# Let's say t_tot has shape (5, 2, 6)\n",
    "t_tot = torch.arange(5 * 2 * 6).reshape(5, 2, 6)\n",
    "\n",
    "# Example index tensors\n",
    "pick = torch.tensor([0, 1, 2])        # shape: [3]\n",
    "jack_cards = torch.tensor([0, 1, 2, 3])  # shape: [4]\n",
    "\n",
    "# Adjust shapes for advanced indexing:\n",
    "# pick -> shape [3, 1]\n",
    "# jack_cards -> shape [1, 4]\n",
    "result = t_tot[pick.unsqueeze(1), 0, jack_cards.unsqueeze(0)]\n",
    "\n",
    "print(\"Result shape:\", result.shape)  # Expected shape: [3, 4]\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'K♣', '2♣', '5♣','9♣','7♣', '3♣', 'J♣', '10♣', '4♣', '1♣', '8♣', '6♣', 'Q♣'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jack_cards.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor([[[0., 0., 0., 0.],\n",
    "#          [0., 0., 0., 0.]],\n",
    "\n",
    "#         [[0., 0., 0., 0.],\n",
    "#          [0., 0., 0., 0.]],\n",
    "\n",
    "#         [[1., 1., 0., 0.],\n",
    "#          [1., 1., 0., 0.]],\n",
    "\n",
    "#         [[0., 0., 0., 0.],\n",
    "#          [0., 0., 0., 0.]],\n",
    "\n",
    "#         [[1., 0., 0., 1.],\n",
    "#          [1., 0., 0., 1.]]], device='cuda:0', dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize tensors\n",
    "t_k = torch.zeros(2, 2, 52)  # Shape (2,2,52)\n",
    "indices = torch.tensor([[51, 48], [51, 49]])  # Shape (2,2)\n",
    "\n",
    "# Use advanced indexing\n",
    "batch_idx = torch.arange(2).unsqueeze(1)  # Creates [[0], [1]]\n",
    "col_idx = torch.arange(2).unsqueeze(0)  # Creates [[0, 1]]\n",
    "\n",
    "t_k[batch_idx, col_idx, indices] = 1\n",
    "\n",
    "# Print result\n",
    "print(t_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['1♥', '8♣', 'J♣', 'K♠']\n",
    "# ['1♦', '1♠', '4♥', 'K♦']\n",
    "# ['2♦', '5♥', '5♠', 'K♥']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t_k[0,0,51] == 1) \n",
    "print(t_k[0,1,48] == 1) \n",
    "print(t_k[1,0,51] == 1)\n",
    "print(t_k[1,1,49] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['10♥']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['5♦', '6♥']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['8♦']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['5♥']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['2♦', '9♠']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['9♣']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['K♥']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n",
    "\n",
    "['1♥', '10♥']\n",
    "['8♦', '9♠', '10♥', 'K♥']\n",
    "['1♥', '5♥', '6♥', '9♣']\n",
    "['2♦', '5♦', '7♥', '9♥']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "output, inverse_indices = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=False, return_inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = torch.tensor([1, 3, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz[inverse_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(2, 3, 4, 5)\n",
    "y = x.view(2, -1, 5)  # Reshape without allocating new memory\n",
    "\n",
    "print(x.data_ptr() == y.data_ptr())  # True (same memory location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "slice_2x52 = torch.randn(2, 3, 5)  # Original tensor\n",
    "slice_2x40 = slice_2x52[:, :, 0:2]    # View (no new memory allocated)\n",
    "print(slice_2x52)\n",
    "print(slice_2x40)\n",
    "del slice_2x52  # Delete original tensor\n",
    "\n",
    "print(slice_2x40)  # Undefined behavior, may cause an error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example: Boolean tensor of shape (N, 10)\n",
    "N = 5  # Example value for N\n",
    "tensor = torch.randint(0, 2, (N, 2), dtype=torch.bool)\n",
    "\n",
    "# Get the indices of True values\n",
    "true_indices = torch.nonzero(tensor)\n",
    "\n",
    "# Count how many True values are in each row\n",
    "true_counts = true_indices[:, 0].bincount(minlength=4)\n",
    "\n",
    "print(true_counts)  # Output: Tensor of shape (N,), showing count of True per row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_counts[:, 0].bincount(minlength=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "N = 10000\n",
    "tensor = torch.randn(N, N)  # Large contiguous tensor\n",
    "tensor_T = tensor.T  # This is non-contiguous\n",
    "\n",
    "# Contiguous Tensor Computation\n",
    "start = time.time()\n",
    "result = torch.matmul(tensor , tensor)  # Element-wise addition\n",
    "end = time.time()\n",
    "print(\"Contiguous Time:\", end - start)\n",
    "\n",
    "# Non-Contiguous Tensor Computation\n",
    "start = time.time()\n",
    "result = torch.matmul(tensor_T , tensor_T)  # Element-wise addition\n",
    "end = time.time()\n",
    "print(\"Non-Contiguous Time:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([0, 0, 0, 0, 1, 2, 1, 0, 0, 0, 0, 1, 2, 1, 0, 2, 4, 2, 2, 4, 4, 0, 2, 0,\n",
    "        0, 2, 2])\n",
    "\n",
    "# Get the indices where the values are nonzero\n",
    "indices = torch.nonzero(tensor).squeeze()\n",
    "\n",
    "# Repeat indices according to their values\n",
    "result = indices.repeat_interleave(tensor[indices])\n",
    "counts_unique = torch.tensor([3, 2, 2, 2, 1, 1, 1, 3, 2, 2, 2, 1, 1, 1, 3, 1, 2, 1, 1, 2, 2, 4, 2, 1, 1, 2, 2])\n",
    "print(result.tolist())  # Output: [4, 5, 5, 6, 11, 12, 12, 13, 15, 15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_unique[tensor>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor[tensor>0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_unique[tensor>0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnts_n_unique[0] \n",
    "# tensor([0, 3, 4], device='cuda:0')\n",
    "# (Pdb) cnts_n_v2[0]\n",
    "# tensor([4, 0, 4, 3], device='cuda:0')\n",
    "# (Pdb) inverse_indices\n",
    "# tensor([2, 0, 2, 1], device='cuda:0')\n",
    "\n",
    "# here is what I need:\n",
    "\n",
    "# i have a tensor zz = ts_n_unique[0]. I need to construct another tensor xx where the first 4 rows zz[3:3+4], next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inverse_indices = torch.tensor([0, 0, 2, 1], device='cuda:0')\n",
    "\n",
    "cnts_n = torch.tensor([2, 2, 4], device='cuda:0')\n",
    "\n",
    "cumsum = torch.cumsum(cnts_n,dim=0)\n",
    "cumsum_max = cumsum[-1] #ts_n_unique[0].shape\n",
    "arange = torch.arange(cumsum_max, device='cuda:0')\n",
    "for i in inverse_indices:\n",
    "    if i == 0:\n",
    "        print(arange[:cumsum[i]])\n",
    "    else:\n",
    "        print(arange[cumsum[i-1]:cumsum[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import line_profiler\n",
    "\n",
    "profiler = line_profiler.LineProfiler()\n",
    "profiler.print_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ['2♠', '4♥', '6♦', 'K♦']\n",
    "# ['1♠', '5♥', '6♥', '8♠']\n",
    "# ['1♥', '5♠', '10♥', 'Q♠']\n",
    "# deads\n",
    "# tensor([3], device='cuda:0') '1♠'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Added missing import\n",
    "import numpy as np\n",
    "\n",
    "# Your tensor\n",
    "tensor = torch.tensor([\n",
    "    0, 0, 32, 2, 0, 0, 32, 2, 0, 0, 32, 2, 1, 1, 33, 3, 1, 1,\n",
    "    33, 3, 1, 1, 33, 3, 1, 1, 33, 3, 4, 4, 34, 6, 4, 4, 34, 6,\n",
    "    4, 4, 34, 6, 4, 4, 34, 6, 5, 5, 35, 7, 5, 5, 35, 7, 5, 5,\n",
    "    35, 7, 8, 8, 36, 10, 8, 8, 36, 10, 8, 8, 36, 10, 8, 8, 36, 10,\n",
    "    8, 8, 36, 10, 9, 9, 37, 11, 9, 9, 37, 11, 9, 9, 37, 11, 9, 9,\n",
    "    37, 11, 9, 9, 37, 11, 12, 12, 38, 14, 12, 12, 38, 14, 12, 12, 38, 14,\n",
    "    13, 13, 39, 15, 16, 16, 40, 17, 16, 16, 40, 17, 18, 18, 0, 20, 18, 18,\n",
    "    0, 20, 18, 18, 0, 20, 19, 19, 1, 21, 19, 19, 1, 21, 22, 22, 41, 23,\n",
    "    22, 22, 41, 23, 22, 22, 41, 23, 22, 22, 41, 23, 24, 24, 42, 25, 24, 24,\n",
    "    42, 25, 24, 24, 42, 25, 26, 26, 43, 27, 26, 26, 43, 27, 26, 26, 43, 27,\n",
    "    26, 26, 43, 27, 26, 26, 43, 27, 28, 28, 44, 29, 30, 30, 22, 31, 30, 30,\n",
    "    22, 31\n",
    "],device='cuda:0')\n",
    "\n",
    "# # Dictionary to store the first occurrence of each value\n",
    "# first_occurrence = {}\n",
    "\n",
    "# # Iterate through the tensor\n",
    "# for index, value in enumerate(tensor.tolist()):  # Ensure iteration over Python integers\n",
    "#     if value not in first_occurrence:  # No need for `.keys()`\n",
    "#         first_occurrence[value] = index\n",
    "# first_occurrence.values()\n",
    "# # Print the first occurrence of each value\n",
    "# for value, index in sorted(first_occurrence.items()):\n",
    "#     print(f\"First occurrence of {value} is at index {index}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[((tensor == i) == True).to(dtype=torch.int8).argmax().item() for i in range(slice_2x40_unique.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted, indices = torch.sort(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor A (size N, with values in the range [1, ..., n])\n",
    "A = torch.tensor([3, 1, 2, 4, 1, 3, 4, 2], dtype=torch.int64)  # Example with N > n, values in [1, 4]\n",
    "\n",
    "# Find the size of n (maximum value in A)\n",
    "n = A.max()\n",
    "\n",
    "# Create a tensor Z of size n to hold the indices\n",
    "Z = torch.zeros(n, dtype=torch.int64)\n",
    "\n",
    "# Populate Z such that A[Z[i]] = i\n",
    "Z[A - 1] = torch.arange(0, A.size(0), dtype=torch.int64)\n",
    "\n",
    "print(Z)  # This will print the result vector Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor A (size N, with values in the range [0, ..., n])\n",
    "A = torch.tensor([3, 0, 2, 4, 0, 3, 4, 1], dtype=torch.int64)  # Example where values are [0, 4]\n",
    "\n",
    "# Find the size of n (maximum value in A)\n",
    "n = A.max()\n",
    "\n",
    "# Create a tensor Z of size n+1 to hold the indices (since 0 is included)\n",
    "Z = torch.zeros(n + 1, dtype=torch.int64)\n",
    "\n",
    "# Populate Z such that A[Z[i]] = i (mapping from value to index)\n",
    "Z[A] = torch.arange(0, A.size(0), dtype=torch.int64)\n",
    "\n",
    "print(Z)  # This will print the result vector Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a 4x3 tensor\n",
    "Z = torch.tensor([[10, 11, 12], \n",
    "                  [20, 21, 22], \n",
    "                  [30, 31, 32], \n",
    "                  [40, 41, 42]])\n",
    "\n",
    "print(\"Original Tensor:\\n\", Z)\n",
    "\n",
    "# Correct way to get the first two rows\n",
    "first_two_rows = Z[:2]\n",
    "print(\"\\nFirst two rows using Z[:2]:\\n\", first_two_rows)\n",
    "\n",
    "# Incorrect way: Z[[1,2]] selects second and third rows\n",
    "wrong_selection = Z[[1, 2]]\n",
    "print(\"\\nWrong selection Z[[1,2]] (second and third rows):\\n\", wrong_selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example t_tot and valid_mask\n",
    "t_tot = torch.randn(5, 5, 10)  # Shape (5, 5, 10)\n",
    "valid_mask = torch.tensor([False, True, False, True, True, False, False, True, False, True])  # Shape (10,)\n",
    "\n",
    "# Extract z\n",
    "z = t_tot[:, :, valid_mask]  # Shape (5, 5, number_of_True)\n",
    "\n",
    "# Find mapping: i -> j\n",
    "j_indices = torch.nonzero(valid_mask, as_tuple=True)[0]  # Indices where valid_mask is True\n",
    "\n",
    "# Now, z[:,:,i] corresponds to t_tot[:,:,j_indices[i]]\n",
    "print(j_indices)  # This gives the mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2, 3])  # Specifies how to partition b\n",
    "b = torch.tensor([1, 2, 3, 4, 5])  # Example b tensor\n",
    "\n",
    "def findsplit(CNT_TOTS):\n",
    "\n",
    "    def mysplit(a,b):\n",
    "\n",
    "        b_splits = torch.split(b, a.tolist())\n",
    "        c = torch.tensor([part.sum() for part in b_splits])\n",
    "        return c \n",
    "    \n",
    "    a,b = CNT_TOTS[0], CNT_TOTS[1]\n",
    "\n",
    "    for i in range(2,len(CNT_TOTS)):\n",
    "        a,b = mysplit(a,b), CNT_TOTS[i]\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(mysplit(a,b))  # Output: tensor([3, 12])  -> (1+2), (3+4+5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "    profile_memory=True,\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    x = torch.randn(10000, 10000, device=\"cuda\")\n",
    "    y = x @ x\n",
    "    del x, y\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example boolean tensor\n",
    "bool_tensor = torch.tensor([False, True, False, True, True, False, False, True])\n",
    "\n",
    "# Get indices of True values\n",
    "\n",
    "\n",
    "true_indices = torch.nonzero(bool_tensor, as_tuple=True)[0]  # Extract indices as a 1D tensor\n",
    "random_index = true_indices[torch.randint(len(true_indices), (1,))].item()\n",
    "\n",
    "\n",
    "# # Pick one randomly\n",
    "# if true_indices.numel() > 0:  # Ensure there are True values\n",
    "    \n",
    "#     print(random_index)  # Randomly selected index\n",
    "# else:\n",
    "#     print(\"No True values found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint(len(true_indices), (1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\U0001F0A1\")  # Using Unicode escape sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creating a sample tensor of shape (2, 3, 4)\n",
    "Z = torch.arange(2 * 3 * 4).reshape(2, 3, 4)\n",
    "\n",
    "# Indices for the second and third coordinates\n",
    "idx2 = torch.tensor([0, 2])\n",
    "idx3 = torch.tensor([2, 3])\n",
    "\n",
    "# Using meshgrid to create indexing\n",
    "grid2, grid3 = torch.meshgrid(idx2, idx3, indexing='ij')\n",
    "\n",
    "# Extracting the subtensor in one go\n",
    "subtensor = Z[:, grid2, grid3]\n",
    "\n",
    "print(subtensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Adds all values from the tensor src into self at the indices specified\n",
    "# Create a target tensor (float type)\n",
    "target = torch.zeros(5, dtype=torch.float32)  # Ensure float32 type\n",
    "\n",
    "# Indices where values should be added\n",
    "index = torch.tensor([2])\n",
    "\n",
    "# Values to add (must have same dtype as target)\n",
    "src = torch.tensor([10, 20, 30, 40, 50], dtype=torch.float32)  # Match dtype\n",
    "\n",
    "# Perform scatter_add_\n",
    "target.scatter_add_(0, index, src)\n",
    "\n",
    "print(target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create two sparse tensors\n",
    "indices1 = torch.tensor([[0, 1], [2, 0]])  # Indices for nonzero values\n",
    "values1 = torch.tensor([3, 4], dtype=torch.float32)\n",
    "size1 = (3, 3)\n",
    "\n",
    "indices2 = torch.tensor([[1, 2], [0, 1]])\n",
    "values2 = torch.tensor([5, 6], dtype=torch.float32)\n",
    "size2 = (3, 3)\n",
    "\n",
    "sparse1 = torch.sparse_coo_tensor(indices1.t(), values1, size1)\n",
    "sparse2 = torch.sparse_coo_tensor(indices2.t(), values2, size2)\n",
    "\n",
    "# Concatenating along dim=0\n",
    "sparse_cat = torch.cat([sparse1, sparse2], dim=0)\n",
    "print(sparse_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Generate two random 3x4 tensors with binary values (0 or 1)\n",
    "tensor1 = torch.randint(0, 2, (3, 4), dtype=torch.bool)\n",
    "tensor2 = torch.randint(0, 2, (3, 4), dtype=torch.bool)\n",
    "\n",
    "# Perform logical AND operation\n",
    "result = torch.logical_and(tensor1, tensor2)\n",
    "\n",
    "print(\"Tensor 1:\")\n",
    "print(tensor1)\n",
    "\n",
    "print(\"\\nTensor 2:\")\n",
    "print(tensor2)\n",
    "\n",
    "print(\"\\nLogical AND Result:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Sparse tensor 1 (3x3)\n",
    "indices1 = torch.tensor([[0, 1], [2, 0]])  # Positions of 1s\n",
    "size1 = (3, 3)\n",
    "\n",
    "# Sparse tensor 2 (3x3)\n",
    "indices2 = torch.tensor([[1, 2], [0, 1]])\n",
    "size2 = (3, 3)\n",
    "\n",
    "# Concatenating along dim=0 (rows) by shifting indices\n",
    "offset = size1[0]  # Shift rows for second tensor\n",
    "indices2_shifted = torch.stack([indices2[0] + offset, indices2[1]])\n",
    "\n",
    "# Combine indices\n",
    "indices_cat = torch.cat([indices1, indices2_shifted], dim=1)\n",
    "size_cat = (size1[0] + size2[0], size1[1])  # New size\n",
    "\n",
    "# Create final sparse tensor (values are all 1s)\n",
    "sparse_cat = torch.sparse_coo_tensor(indices_cat, torch.ones(indices_cat.shape[1]), size_cat)\n",
    "\n",
    "print(sparse_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example data\n",
    "list_mask_cur = [10, 20, 30, 40, 50]\n",
    "t_mask_cur = torch.tensor([True, False, True, False, True])\n",
    "\n",
    "# Filtering values where t_mask_cur is True\n",
    "filtered_list = [val for val, mask in zip(list_mask_cur, t_mask_cur.tolist()) if mask]\n",
    "\n",
    "print(filtered_list)  # Output: [10, 30, 50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input tensor (values must be non-negative integers)\n",
    "x = torch.tensor([0, 1, 1, 2, 2, 2, 3, 3, 3, 3])\n",
    "\n",
    "# Compute bin counts\n",
    "counts = torch.bincount(x)\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a random 2x2x4 tensor with 0s and 1s\n",
    "tensor = torch.randint(0, 2, (2, 2, 4))\n",
    "\n",
    "# Find nonzero indices\n",
    "nonzero_indices = torch.nonzero(tensor)\n",
    "mask = nonzero_indices[:, 1] == 0 \n",
    "\n",
    "print(\"Tensor:\\n\", tensor)\n",
    "print(\"\\nNonzero indices:\\n\", nonzero_indices)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_indices[~mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+torch.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "t_z = torch.rand(5, 2, 3)  # Example 3D tensor\n",
    "t_lst_pick0 = torch.tensor([True, False, True, False, True])  # Boolean mask\n",
    "\n",
    "t_pool0 = t_z[t_lst_pick0, 0, :]\n",
    "t_pool0[t_pool0 > 0] = 1\n",
    "\n",
    "print(torch.equal(t_z[t_lst_pick0, 0, :], t_pool0))  # False → `t_z` remains unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_z[t_lst_pick0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y=3):\n",
    "\n",
    "    def f2(y):\n",
    "        y += 3\n",
    "\n",
    "    f2(y)    \n",
    "    return y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = {'1':1,'2':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(ss.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_occurrence_indices(strings):\n",
    "\n",
    "    index_map = {}\n",
    "    cur = 0\n",
    "    for string in strings:\n",
    "        if string not in index_map:\n",
    "            index_map[string] = cur\n",
    "            cur = cur+1\n",
    "    res = dict(index_map)\n",
    "    return [res[key] for key in strings]\n",
    "\n",
    "# Example Usage\n",
    "strings = [\"2\", \"2\", \"3\", \"12\",\"2\"]\n",
    "first_occurrence_indices(strings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = [1,2]\n",
    "aa.extend([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "X = torch.tensor([True, False, True])  # Shape (10,)\n",
    "Y = torch.tensor([True, \n",
    "                  False, False, False, False, False, False, True, True])  # Shape (20,)\n",
    "\n",
    "# Get indices of True values in Y\n",
    "true_indices = torch.nonzero(Y, as_tuple=True)[0]\n",
    "\n",
    "# Modify Y: Set the i-th True value to False if X[i] is False\n",
    "Y[true_indices] = X\n",
    "\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensors\n",
    "tensor_1 = torch.tensor([[1, 0, 1], [0, 1, 1]])  # Shape (2, 3)\n",
    "tensor_2 = torch.tensor([[1, 1, 0, 1], [0, 1, 1, 0]])  # Shape (2, 4)\n",
    "\n",
    "# Expand dimensions for broadcasting\n",
    "tensor_1_exp = tensor_1.unsqueeze(2)  # Shape: (2, 3, 1)\n",
    "tensor_2_exp = tensor_2.unsqueeze(1)  # Shape: (2, 1, 4)\n",
    "\n",
    "# Compute Z[i, j, k] = 1 iff tensor_1[i, j] == 1 and tensor_2[i, k] == 1\n",
    "Z = (tensor_1_exp & tensor_2_exp).int()  # Shape: (2, 3, 4)\n",
    "\n",
    "# Print result\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create example tensor (shape N=2, M=3, 5)\n",
    "t_scr_tmp = torch.ones(2, 3, 5)\n",
    "tm_clb = t_scr_tmp[:,:,0]>=1  # Example mask (shape N=2)\n",
    "\n",
    "# One-line modification\n",
    "t_scr_tmp[tm_clb, :, [0, 1]] = 0  # or t_scr_tmp[tm_clb, :, 0:2] = 0\n",
    "\n",
    "print(t_scr_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a random tensor of shape (N=2, M=3, 5)\n",
    "Z = torch.randint(0, 2, size=(2, 3, 5)).float()\n",
    "print(\"Original tensor:\")\n",
    "print(Z)\n",
    "\n",
    "# Find indices where Z[i,j,0] >= 7\n",
    "mask = Z[:, :, 0] >= 1\n",
    "print(\"\\nMask (where Z[i,j,0] >= 7):\")\n",
    "print(mask)\n",
    "\n",
    "# Set Z[i,j,4] to 1 where mask is True\n",
    "Z[:,:,4][mask] = 0\n",
    "\n",
    "print(\"\\nModified tensor:\")\n",
    "print(Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = torch.tensor([10, 20, 30, 40], dtype=torch.float32)\n",
    "indices = torch.tensor([0, 1, 0, 1], dtype=torch.int64)  # assign values to bin 0 or 1\n",
    "\n",
    "result = torch.zeros(2)\n",
    "result.scatter_add_(0, indices, values)\n",
    "# result = [10+30, 20+40] = [40, 60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import stat\n",
    "\n",
    "# Set the path to the folder containing subfolders\n",
    "base_dir = Path(\"data\")\n",
    "\n",
    "# Loop through all subdirectories\n",
    "for subfolder in base_dir.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        zip_path = base_dir / f\"{subfolder.name}.zip\"\n",
    "        \n",
    "        # Create zip archive\n",
    "        shutil.make_archive(str(zip_path.with_suffix('')), 'zip', root_dir=subfolder)\n",
    "        \n",
    "        # Remove the original folder after zipping\n",
    "        shutil.rmtree(subfolder)\n",
    "\n",
    "        # Make the .zip file read-only\n",
    "        os.chmod(zip_path, stat.S_IREAD | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import stat\n",
    "\n",
    "# Define paths\n",
    "unzipped_dir = Path(\"Unzipped\")\n",
    "zipped_dir = Path(\"Zipped\")\n",
    "\n",
    "# Create destination if it doesn't exist\n",
    "zipped_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Loop through all subfolders in UnzippedData\n",
    "for subfolder in unzipped_dir.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        zip_filename = zipped_dir / f\"{subfolder.name}.zip\"\n",
    "        \n",
    "        # Create zip archive in ZippedData\n",
    "        shutil.make_archive(str(zip_filename.with_suffix('')), 'zip', root_dir=subfolder)\n",
    "\n",
    "        # Make zip file read-only\n",
    "        os.chmod(zip_filename, stat.S_IREAD | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "        # Remove original folder\n",
    "        shutil.rmtree(subfolder)\n",
    "\n",
    "        print(f\"Zipped and removed: {subfolder.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def get_batch_end_indices(tc_scr, max_tokens):\n",
    "    \n",
    "\n",
    "    return end_indices\n",
    "\n",
    "\n",
    "def get_batch_ranges_from_ends(end_indices):\n",
    "    # Prepend 0 as the start of the first batch\n",
    "    start_indices = torch.cat([torch.tensor([0], device=end_indices.device), end_indices[:-1]])\n",
    "    return list(zip(start_indices.tolist(), end_indices.tolist()))\n",
    "\n",
    "tc_scr = torch.tensor([100, 200, 300, 150, 250, 500, 100])\n",
    "max_tokens = 1000\n",
    "\n",
    "cumsum = tc_scr.cumsum(0)\n",
    "total_tokens = cumsum[-1].item()\n",
    "\n",
    "# Define thresholds: 1000, 2000, ..., up to total_tokens\n",
    "thresholds = torch.arange(max_tokens, total_tokens + max_tokens, max_tokens, device=tc_scr.device)\n",
    "\n",
    "# searchsorted gives first index where cumsum >= threshold\n",
    "end_indices = torch.searchsorted(cumsum, thresholds, right=True)\n",
    "start_indices = torch.cat([torch.tensor([0], device=end_indices.device), end_indices[:-1]])\n",
    "ranges = list(zip(start_indices.tolist(), end_indices.tolist()))\n",
    "\n",
    "for s, e in ranges:\n",
    "    print(f\"Batch: start={s}, end={e}, sum={tc_scr[s:e].sum().item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = torch.tensor([5, 3, 7], dtype=torch.long)\n",
    "torch.floor(torch.rand_like(Z, dtype=torch.float) * Z).to(torch.int32) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "Z = torch.tensor([5, 3, 7], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input\n",
    "t_org = torch.tensor([1, 2, 3, 4, 5, 6])      # 1D tensor\n",
    "t_bsz = torch.tensor([2, 3, 1])           # Block t_bsz: s₁, s₂, s₃\n",
    "t_rpt = torch.tensor([1, 3, 2])         # Repeat counts: a₁, a₂, a₃\n",
    "\n",
    "# Compute start indices of each block\n",
    "t_bgn = torch.cumsum(torch.cat([torch.tensor([0]), t_bsz[:-1]]), dim=0)\n",
    "\n",
    "# Create an index tensor for each element's original position\n",
    "# e.g., block 0 repeated 1 time, block 1 repeated 3 times, block 2 repeated 2 times\n",
    "t_blk = torch.repeat_interleave(t_bgn, t_rpt)\n",
    "# tensor([0, 2, 2, 2, 5, 5])\n",
    "t_bls = torch.repeat_interleave(t_bsz, t_rpt)\n",
    "## tensor([2, 3, 3, 3, 1, 1])\n",
    "\n",
    "## identify each block using its first index\n",
    "t_blk = t_blk.repeat_interleave(t_bls) \n",
    "## b0,b0,b1,b1,b1,b1,b1,b1,b1,b1,b1,b2,b2\n",
    "## tensor([0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5])\n",
    "\n",
    "## shift starting value \n",
    "## tensor([ 0,  0,  2,  2,  2,  5,  5,  5,  8,  8,  8, 11, 12])\n",
    "tmp2 = torch.cumsum(torch.cat([torch.tensor([0]), t_bls[:-1]]), dim=0).repeat_interleave(t_bls)\n",
    "total = t_blk.shape[0]\n",
    "positions = torch.arange(total) - tmp2\n",
    "indices = positions+t_blk\n",
    "res = t_org[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cumsum(torch.cat([torch.tensor([0]), t_bls[:-1]]), dim=0)\n",
    "# \\texttt{cumsum(0,t\\_bls[:-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example input\n",
    "t_org = torch.tensor([1, 2, 3, 4, 5, 6])      # 1D tensor\n",
    "t_bsz = torch.tensor([2, 3, 1])           # Block t_bsz: s₁, s₂, s₃\n",
    "t_rpt = torch.tensor([1, 3, 2])         # Repeat counts: a₁, a₂, a₃\n",
    "\n",
    "# Compute start t_idx of each block\n",
    "t_bgn = torch.repeat_interleave(torch.cumsum(torch.cat([torch.tensor([0]), t_bsz[:-1]]), dim=0), t_rpt)\n",
    "\n",
    "# Create an index tensor for each element's original position\n",
    "# e.g., block 0 repeated 1 time, block 1 repeated 3 times, block 2 repeated 2 times\n",
    "# t_blk = torch.repeat_interleave(t_bgn, t_rpt)\n",
    "# tensor([0, 2, 2, 2, 5, 5])\n",
    "t_bls = torch.repeat_interleave(t_bsz, t_rpt)\n",
    "## tensor([2, 3, 3, 3, 1, 1])\n",
    "\n",
    "## identify each block using its first index\n",
    "t_blk = torch.repeat_interleave(t_bgn, t_bls) \n",
    "## b0,b0,b1,b1,b1,b1,b1,b1,b1,b1,b1,b2,b2\n",
    "## tensor([0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5])\n",
    "\n",
    "## shift starting value \n",
    "## tensor([ 0,  0,  2,  2,  2,  5,  5,  5,  8,  8,  8, 11, 12])\n",
    "t_csm = torch.cumsum(torch.cat([torch.tensor([0]), t_bls[:-1]]), dim=0).repeat_interleave(t_bls)\n",
    "i_blk = t_blk.shape[0]\n",
    "t_pos = torch.arange(i_blk) - t_csm\n",
    "t_idx = t_pos+t_blk\n",
    "t_rbk = t_org[t_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(total) - positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Example tensor with consecutive duplicates\n",
    "x = torch.tensor([1, 1, 2, 2, 2, 3, 1, 1, 4])\n",
    "\n",
    "# Apply unique_consecutive\n",
    "unique_vals = torch.unique_consecutive(x, return_inverse=True)\n",
    "\n",
    "print(unique_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "def get_var_name(var):\n",
    "    frame = inspect.currentframe().f_back\n",
    "    for name, val in frame.f_locals.items():\n",
    "        if val is var:\n",
    "            return name\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 42\n",
    "y = x\n",
    "print(get_var_name(x))  # Output: 'x'\n",
    "\n",
    "t_sid = [1, 2, 3]\n",
    "print(get_var_name(t_sid))  # Output: 't_sid'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "weights = torch.tensor([[0, .10, .3, .0100],[1000, 10, .3, .0100]], dtype=torch.float) # create a tensor of weights\n",
    "torch.multinomial(weights, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extsampling(t_edg, c_edg, t_sgm):\n",
    "\n",
    "    res             = torch.zeros_like(t_sgm, dtype=torch.bool)\n",
    "    t_idx           = torch.argsort(t_edg)\n",
    "    t_inv           = torch.empty_like(t_idx)\n",
    "    t_inv[t_idx]    = torch.arange(len(t_idx))\n",
    "\n",
    "    st_sgm          = t_sgm[t_idx]\n",
    "    i_max           = c_edg.max()\n",
    "    i_num           = c_edg.shape[0]\n",
    "    t_msk = torch.arange(i_max).unsqueeze(0) < c_edg.unsqueeze(1)\n",
    "    \n",
    "    t_mtx           = torch.zeros_like(t_msk, dtype=t_sgm.dtype)\n",
    "    t_mtx[t_msk]    = t_sgm\n",
    "\n",
    "    t_smp = torch.multinomial(t_mtx, num_samples=1).squeeze(1) \n",
    "    t_gps = torch.cat([torch.tensor([0]), c_edg.cumsum(0)[:-1]]) # group starts \n",
    "    res[t_gps + t_smp] = True \n",
    "    return res[t_inv]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor\n",
    "x = torch.tensor([[3, 100],\n",
    "                  [1, 200],\n",
    "                  [2, 300]])\n",
    "\n",
    "# Sort by column 0\n",
    "sort_idx          = torch.argsort(x[:, 0])\n",
    "inv_idx           = torch.empty_like(sort_idx)\n",
    "inv_idx[sort_idx] = torch.arange(len(sort_idx))\n",
    "\n",
    "sorted_x    = x[sort_idx]\n",
    "recovered_x = sorted_x[inv_idx]\n",
    "\n",
    "print(\"Sorted x:\\n\", sorted_x)\n",
    "print(\"Recovered x:\\n\", recovered_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "probs = torch.tensor([0.1, 0.9, 0.3, 0.7, 0.2, 0.2, 0.6, 1.0, 0.5, 0.5])\n",
    "c_edg = torch.tensor([2, 2, 3, 1, 2])\n",
    "\n",
    "i_max = c_edg.max()\n",
    "i_num = len(c_edg)\n",
    "\n",
    "t_msk = torch.zeros((i_num, i_max), dtype=torch.bool)\n",
    "idx = torch.arange(i_max)\n",
    "t_msk = idx.unsqueeze(0) < c_edg.unsqueeze(1)\n",
    "\n",
    "t_mtx = torch.zeros_like(t_msk, dtype=probs.dtype)\n",
    "t_mtx[t_msk] = probs\n",
    "\n",
    "# Step 2: Sample one index per group from padded probs\n",
    "sampled_local = torch.multinomial(t_mtx, num_samples=1).squeeze(1)  # [i_num]\n",
    "\n",
    "# Step 3: Convert to global indices\n",
    "group_start = torch.cat([torch.tensor([0]), c_edg.cumsum(0)[:-1]])\n",
    "global_indices = group_start + sampled_local\n",
    "\n",
    "print(global_indices)  # shape: [i_num], each index from global `probs`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(padded_probs, num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([0,0, 2, 2, 3, 7])\n",
    "\n",
    "# Find where values change\n",
    "is_new = torch.ones_like(x, dtype=torch.bool)\n",
    "is_new[1:] = x[1:] != x[:-1]\n",
    "\n",
    "# Assign group ids\n",
    "group_ids = torch.cumsum(is_new, dim=0) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Dummy data\n",
    "X = np.random.rand(10, 5).astype(np.float32)\n",
    "dtest = xgb.DMatrix(X)\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"base_score\": 0.0,\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n",
    "\n",
    "model = xgb.train(params, dtrain=dtest, num_boost_round=0)\n",
    "\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.DMatrix at 0x2cb8e4df790>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 0, 1, 0], dtype=torch.bool)\n",
    "t2 = torch.randint(0, 2, (4, 4), dtype=torch.bool)\n",
    "\n",
    "result = torch.logical_and(t1, t2)\n",
    "print(result.shape)  # torch.Size([4, 12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True, False],\n",
       "        [False, False,  True, False],\n",
       "        [False, False,  True, False],\n",
       "        [ True, False, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False,  True, False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True,  True],\n",
       "        [False, False,  True,  True],\n",
       "        [False, False,  True, False],\n",
       "        [ True, False, False, False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Dummy features\n",
    "X = np.random.rand(100, 10).astype(np.float32)\n",
    "y = np.zeros(100, dtype=np.float32)  # all targets are 0\n",
    "\n",
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"max_depth\": 1,\n",
    "    \"learning_rate\": 1.0,\n",
    "    \"nthread\": 1\n",
    "}\n",
    "\n",
    "# One boosting round is enough to learn constant output\n",
    "model = xgb.train(params, dtrain, num_boost_round=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.Booster at 0x2cb4ae4b400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
